/**
 * useGeminiLive - Gemini Live å¤šæ¨¡æ€å¯¹è¯ Hookï¼ˆç»„åˆå™¨ï¼‰
 *
 * è¿™æ˜¯ä¸»å…¥å£ hookï¼Œç»„åˆæ‰€æœ‰å­æ¨¡å—ï¼š
 * - useGeminiSession: WebSocket è¿æ¥ç®¡ç†
 * - useAudioInput: éº¦å…‹é£å½•åˆ¶
 * - useVideoInput: æ‘„åƒå¤´æ•è·
 * - useAudioOutput: éŸ³é¢‘æ’­æ”¾
 * - useTranscript: è½¬å½•ç®¡ç†
 * - useSessionAnalytics: åŸ‹ç‚¹è¿½è¸ª
 *
 * ä¿æŒåŸæœ‰ API å…¼å®¹ï¼Œè°ƒç”¨æ–¹æ— éœ€ä¿®æ”¹
 */

import { useCallback, useEffect, useRef } from 'react';
import type { LiveServerMessage, FunctionDeclaration } from '@google/genai';

// Core
import { useGeminiSession, fetchGeminiToken } from './core/useGeminiSession';
import { handleServerContent } from './core/messageHandlers';

// Media
import { useAudioInput } from './media/useAudioInput';
import { useVideoInput } from './media/useVideoInput';
import { useAudioOutput } from './media/useAudioOutput';

// Features
import { useTranscript } from './features/useTranscript';
import { useSessionAnalytics } from './features/useSessionAnalytics';

// Types
import type { ToolCall, ToolCallEvent, TranscriptEntry } from './types';
import { devLog } from './utils';

// ============================================================================
// Types
// ============================================================================

export type GeminiLiveStatus = 'disconnected' | 'connecting' | 'connected' | 'error';

interface UseGeminiLiveOptions {
  systemInstruction?: string;
  tools?: FunctionDeclaration[];
  onTranscriptUpdate?: (transcript: TranscriptEntry[]) => void;
  onMessage?: (message: LiveServerMessage) => void;
  onTurnComplete?: () => void;
  onToolCall?: (toolCall: ToolCallEvent) => void;
  enableCamera?: boolean;
  enableMicrophone?: boolean;
}

// ============================================================================
// Main Hook
// ============================================================================

export function useGeminiLive(options: UseGeminiLiveOptions = {}) {
  const {
    systemInstruction,
    tools,
    onTranscriptUpdate,
    onMessage,
    onTurnComplete,
    onToolCall,
    enableCamera = false,
    enableMicrophone = false,
  } = options;

  // ============================================================================
  // Sub-hooks
  // ============================================================================

  // Analytics (must be first to track initial state)
  const analytics = useSessionAnalytics();

  // Audio output (for playing AI responses)
  const audioOutput = useAudioOutput({
    onPlaybackComplete: () => {
      // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ’­æ”¾å®Œæˆçš„é€»è¾‘
    },
  });

  // Transcript management
  const transcriptManager = useTranscript({
    onUpdate: onTranscriptUpdate,
  });

  // Refs for callbacks (é¿å…é—­åŒ…é—®é¢˜)
  const onTurnCompleteRef = useRef<(() => void) | null>(onTurnComplete ?? null);
  const onToolCallRef = useRef<((toolCall: ToolCallEvent) => void) | null>(onToolCall ?? null);

  useEffect(() => {
    onTurnCompleteRef.current = onTurnComplete ?? null;
  }, [onTurnComplete]);

  useEffect(() => {
    onToolCallRef.current = onToolCall ?? null;
  }, [onToolCall]);

  // Session management (core)
  const session = useGeminiSession({
    onMessage: (message: LiveServerMessage) => {
      // è°ƒç”¨å¤–éƒ¨æ¶ˆæ¯å¤„ç†å™¨
      onMessage?.(message);

      // ä½¿ç”¨æ¶ˆæ¯å¤„ç†å™¨å¤„ç†æœåŠ¡å™¨å†…å®¹
      if (message.serverContent) {
        handleServerContent(message, {
          onInterrupt: () => {
            audioOutput.stop();
          },
          onTurnComplete: () => {
            audioOutput.markTurnComplete();  // é‡ç½® isSpeaking çŠ¶æ€
            onTurnCompleteRef.current?.();
          },
          onInputTranscription: (text: string) => {
            transcriptManager.addUserEntry(text);
          },
          onOutputTranscription: (text: string) => {
            transcriptManager.addAssistantEntry(text);
          },
          onToolCall: (toolCall: ToolCall) => {
            devLog('ğŸ”§ Tool call received:', toolCall);

            if (toolCall?.functionCalls && toolCall.functionCalls.length > 0) {
              const functionCall = toolCall.functionCalls[0];
              const functionName = functionCall.name;
              const args = functionCall.args;

              devLog('ğŸ“ Function called:', functionName, args);

              if (onToolCallRef.current) {
                onToolCallRef.current({ functionName, args });
              }

              // Send function response back to AI
              session.sendToolResponse({
                functionResponses: [
                  {
                    id: functionCall.id,
                    name: functionName,
                    response: { success: true },
                  },
                ],
              });
            }
          },
          onAudioData: async (data: string) => {
            try {
              await audioOutput.ensureReady();
              audioOutput.playAudio(data);
            } catch (err) {
              devLog('âš ï¸ Audio playback error:', err);
            }
          },
          onTextContent: (text: string) => {
            transcriptManager.addAssistantEntry(text);
          },
          session: session.sessionRef.current,
        });
      }
    },
    onConnected: () => {
      analytics.trackConnect();
    },
    onDisconnected: () => {
      // Cleanup handled in disconnect
    },
    onError: (error: string) => {
      devLog('Session error:', error);
    },
  });

  // Audio input (microphone)
  // è§£æ„å‡ºç¨³å®šçš„å­—æ®µï¼Œé¿å…ä¾èµ–æ•´ä¸ªå¯¹è±¡å¯¼è‡´ useCallback/useEffect é‡å¤è§¦å‘
  const audioInput = useAudioInput({
    onAudioData: (base64Audio: string) => {
      session.sendRealtimeInput({
        media: {
          mimeType: 'audio/pcm;rate=16000',
          data: base64Audio,
        },
      });
    },
    onError: (error: string) => {
      devLog('Audio input error:', error);
    },
  });
  const {
    isRecording: audioIsRecording,
    start: audioStart,
    stop: audioStop,
    error: audioError,
    audioStream,
  } = audioInput;

  // Video input (camera)
  // è§£æ„å‡ºç¨³å®šçš„å­—æ®µï¼Œé¿å…ä¾èµ–æ•´ä¸ªå¯¹è±¡å¯¼è‡´ useCallback/useEffect é‡å¤è§¦å‘
  const videoInput = useVideoInput({
    onVideoFrame: (base64Jpeg: string) => {
      session.sendRealtimeInput({
        media: {
          mimeType: 'image/jpeg',
          data: base64Jpeg,
        },
      });
    },
    onError: (error: string) => {
      devLog('Video input error:', error);
    },
  });
  const {
    isEnabled: videoIsEnabled,
    start: videoStart,
    stop: videoStop,
    error: videoError,
    videoStream,
    videoRef,
    canvasRef,
    startFrameCapture,
    stopFrameCapture,
  } = videoInput;

  // ============================================================================
  // Composed Actions
  // ============================================================================

  /**
   * è¿æ¥ Gemini Live
   * @param customSystemInstruction - è‡ªå®šä¹‰ç³»ç»ŸæŒ‡ä»¤
   * @param customTools - è‡ªå®šä¹‰å·¥å…·åˆ—è¡¨
   * @param prefetchedToken - é¢„è·å–çš„ Gemini token
   * @param voiceName - AI å£°éŸ³åç§°ï¼ˆå¦‚ 'Puck', 'Kore'ï¼‰
   */
  const connect = useCallback(async (
    customSystemInstruction?: string,
    customTools?: FunctionDeclaration[],
    prefetchedToken?: string,
    voiceName?: string
  ) => {
    // é‡ç½®ç»Ÿè®¡
    analytics.resetStats();

    // é¢„åˆå§‹åŒ– AudioContextï¼ˆå¿…é¡»åœ¨ç”¨æˆ·äº¤äº’ä¸Šä¸‹æ–‡ä¸­ï¼‰
    devLog('ğŸ”Š Pre-initializing AudioContext...');
    await audioOutput.ensureReady();
    devLog('âœ… AudioContext ready');

    // å»ºç«‹è¿æ¥
    await session.connect(
      {
        systemInstruction: customSystemInstruction || systemInstruction,
        tools: customTools || tools,
        voiceName,
      },
      prefetchedToken
    );
  }, [systemInstruction, tools, session, audioOutput, analytics]);

  /**
   * æ–­å¼€è¿æ¥å¹¶æ¸…ç†æ‰€æœ‰èµ„æº
   */
  const disconnect = useCallback(() => {
    devLog('ğŸ”Œ Disconnecting Gemini Live...');

    // åŸ‹ç‚¹
    analytics.trackDisconnect();

    // å…³é—­ session
    session.disconnect();

    // åœæ­¢éº¦å…‹é£
    audioStop();

    // åœæ­¢æ‘„åƒå¤´
    videoStop();

    // æ¸…ç†éŸ³é¢‘è¾“å‡º
    audioOutput.cleanup();

    devLog('âœ… Gemini Live disconnected and cleaned up');
  }, [session, audioStop, videoStop, audioOutput, analytics]);

  /**
   * åˆ‡æ¢éº¦å…‹é£
   * ä½¿ç”¨è§£æ„çš„ç¨³å®šå­—æ®µä½œä¸ºä¾èµ–ï¼Œé¿å…æ¯æ¬¡æ¸²æŸ“éƒ½é‡å»ºå‡½æ•°
   */
  const toggleMicrophone = useCallback(async () => {
    if (audioIsRecording) {
      audioStop();
      analytics.trackMicToggle(false);
    } else {
      // ç¡®ä¿ AudioContext å·²å‡†å¤‡
      await audioOutput.ensureReady();
      await audioStart();
      analytics.trackMicToggle(true);
    }
  }, [audioIsRecording, audioStart, audioStop, audioOutput, analytics]);

  /**
   * åˆ‡æ¢æ‘„åƒå¤´
   * ä½¿ç”¨è§£æ„çš„ç¨³å®šå­—æ®µä½œä¸ºä¾èµ–ï¼Œé¿å…æ¯æ¬¡æ¸²æŸ“éƒ½é‡å»ºå‡½æ•°
   */
  const toggleCamera = useCallback(async () => {
    if (videoIsEnabled) {
      videoStop();
      analytics.trackCameraToggle(false);
    } else {
      await audioOutput.ensureReady();
      await videoStart();
      analytics.trackCameraToggle(true);
    }
  }, [videoIsEnabled, videoStart, videoStop, audioOutput, analytics]);

  /**
   * å‘é€æ–‡æœ¬æ¶ˆæ¯
   * æ³¨æ„ï¼šä½¿ç”¨ session.isConnected å’Œ session.sendRealtimeInput ä½œä¸ºä¾èµ–
   * è€Œä¸æ˜¯æ•´ä¸ª session å¯¹è±¡ï¼Œé¿å…å› å¯¹è±¡å¼•ç”¨å˜åŒ–å¯¼è‡´å‡½æ•°é¢‘ç¹é‡å»º
   */
  const sendTextMessage = useCallback((text: string) => {
    if (session.isConnected) {
      session.sendRealtimeInput({ text });
      if (import.meta.env.DEV) {
        console.log('ğŸ“¤ [GeminiLive] å‘é€æ–‡æœ¬:', text.substring(0, 60) + (text.length > 60 ? '...' : ''));
      }
    } else if (import.meta.env.DEV) {
      console.warn('âš ï¸ [GeminiLive] å‘é€å¤±è´¥: è¿æ¥å·²æ–­å¼€');
    }
  }, [session.isConnected, session.sendRealtimeInput]);

  /**
   * è®¾ç½® onTurnComplete å›è°ƒ
   */
  const setOnTurnComplete = useCallback((handler: (() => void) | null | undefined) => {
    onTurnCompleteRef.current = handler ?? null;
  }, []);

  // ============================================================================
  // Auto-enable Effects
  // ä½¿ç”¨è§£æ„çš„ç¨³å®šå­—æ®µä½œä¸ºä¾èµ–ï¼Œé¿å…æ¯æ¬¡æ¸²æŸ“éƒ½è§¦å‘ effect
  // ============================================================================

  // è¿æ¥åè‡ªåŠ¨å¯ç”¨æ‘„åƒå¤´
  useEffect(() => {
    if (session.isConnected && enableCamera && !videoIsEnabled) {
      queueMicrotask(() => {
        toggleCamera();
      });
    }
  }, [session.isConnected, enableCamera, videoIsEnabled, toggleCamera]);

  // è¿æ¥åè‡ªåŠ¨å¯ç”¨éº¦å…‹é£
  useEffect(() => {
    if (session.isConnected && enableMicrophone && !audioIsRecording) {
      queueMicrotask(() => {
        toggleMicrophone();
      });
    }
  }, [session.isConnected, enableMicrophone, audioIsRecording, toggleMicrophone]);

  // è¿æ¥åå¼€å§‹è§†é¢‘å¸§æ•è·
  useEffect(() => {
    if (session.isConnected && videoIsEnabled) {
      startFrameCapture();
    } else {
      stopFrameCapture();
    }
  }, [session.isConnected, videoIsEnabled, startFrameCapture, stopFrameCapture]);

  // ============================================================================
  // Return (ä¿æŒåŸæœ‰ API å…¼å®¹)
  // ============================================================================

  return {
    // State
    isConnected: session.isConnected,
    isRecording: audioIsRecording,
    isSpeaking: audioOutput.isSpeaking,
    // åˆå¹¶æ‰€æœ‰é”™è¯¯ï¼Œé¿å…ä¸¢å¤±ä¿¡æ¯
    error: [session.error, audioError, videoError].filter(Boolean).join('; ') || null,
    transcript: transcriptManager.transcript,
    cameraEnabled: videoIsEnabled,
    videoStream,
    audioStream,

    // Actions
    connect,
    disconnect,
    toggleMicrophone,
    toggleCamera,
    sendTextMessage,
    setOnTurnComplete,

    // Refs for UI
    videoRef,
    canvasRef,
  };
}

// Re-export for convenience
export { fetchGeminiToken };
export type { TranscriptEntry, ToolCallEvent };
