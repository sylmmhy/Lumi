import { useEffect, useRef, useState } from 'react';

interface UseVoiceActivityDetectionOptions {
  enabled?: boolean;
  /** ä¸Šå‡é˜ˆå€¼ï¼ˆ0-255ï¼‰ï¼Œè¶…è¿‡æ‰è®¤ä¸ºå¯èƒ½åœ¨è¯´è¯ï¼Œé»˜è®¤ 30 */
  threshold?: number;
  smoothingTimeConstant?: number; // 0-1, default 0.8
  fftSize?: number; // Must be power of 2, default 2048
  /** æœ€çŸ­æŒç»­æ—¶é—´ï¼ˆmsï¼‰ï¼ŒæŒç»­è¶…è¿‡æ­¤æ—¶é—´æ‰è®¤å®šä¸º"è¯´è¯"ï¼Œé»˜è®¤ 250msã€‚
   * ç¯ç«æ¨¡å¼ç­‰éœ€è¦å¿«é€Ÿå“åº”çš„åœºæ™¯å¯è®¾ä¸ºæ›´ä½å€¼ï¼ˆå¦‚ 100msï¼‰ */
  minSpeechDuration?: number;
}

/**
 * Voice Activity Detection (VAD) hook
 *
 * - ä½¿ç”¨å¸¦é€šæ»¤æ³¢ï¼ˆ300-4000Hzï¼‰è¿‡æ»¤ç¯å¢ƒåº•å™ª/æ°´æµç­‰ä½é¢‘ä¸è¶…é«˜é¢‘æ‚éŸ³
 * - åŒé˜ˆå€¼ï¼ˆä¸Šå‡/ä¸‹é™ï¼‰é¿å…é”®ç›˜ç­‰çŸ­è„‰å†²æŠŠçŠ¶æ€å¡åœ¨â€œæ­£åœ¨è¯´è¯â€
 * - æœ€çŸ­æŒç»­æ—¶é—´é—¨æ§›ï¼ˆé»˜è®¤ 250msï¼‰è¿‡æ»¤çŸ­ä¿ƒå™ªå£°
 *
 * @param {MediaStream | null} mediaStream - éº¦å…‹é£æµ
 * @param {UseVoiceActivityDetectionOptions} options - VAD é…ç½®
 * @returns {{ isSpeaking: boolean; currentVolume: number; lastSpeakingTime: Date | null }} VAD çŠ¶æ€
 */
export function useVoiceActivityDetection(
  mediaStream: MediaStream | null,
  options: UseVoiceActivityDetectionOptions = {}
) {
  const {
    enabled = true,
    threshold = 30,
    smoothingTimeConstant = 0.8,
    fftSize = 2048,
    minSpeechDuration = 250,
  } = options;

  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentVolume, setCurrentVolume] = useState(0);
  const [lastSpeakingTime, setLastSpeakingTime] = useState<number>(0);

  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const highpassRef = useRef<BiquadFilterNode | null>(null);
  const lowpassRef = useRef<BiquadFilterNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const speakingRef = useRef(false); // avoid effect churn on speaking toggles
  const speechStartRef = useRef<number | null>(null);

  useEffect(() => {
    if (!enabled || !mediaStream) {
      return;
    }

    // Create AudioContext and AnalyserNode
    const audioContext = new AudioContext();

    // ğŸ”§ ä¿®å¤ï¼šç§»åŠ¨ç«¯æµè§ˆå™¨ AudioContext å¯èƒ½å¤„äº suspended çŠ¶æ€
    // å¿…é¡» resume() æ‰èƒ½å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œå¦åˆ™ getByteFrequencyData å…¨æ˜¯ 0
    if (audioContext.state === 'suspended') {
      console.log('ğŸ”Š [VAD] AudioContext is suspended, resuming...');
      audioContext.resume().then(() => {
        console.log('ğŸ”Š [VAD] AudioContext resumed:', audioContext.state);
      }).catch((err) => {
        console.error('ğŸ”Š [VAD] AudioContext resume failed:', err);
      });
    }
    console.log('ğŸ”Š [VAD] AudioContext state:', audioContext.state, '| MediaStream active:', mediaStream.active,
      '| Tracks:', mediaStream.getTracks().map(t => `${t.kind}:${t.readyState}:${t.enabled}`).join(','));

    const analyser = audioContext.createAnalyser();
    analyser.fftSize = fftSize;
    analyser.smoothingTimeConstant = smoothingTimeConstant;

    // å¸¦é€šæ»¤æ³¢ï¼šè¿‡æ»¤ <300Hz çš„ä½é¢‘å—¡é¸£å’Œ >4kHz çš„å°–é”å™ªå£°
    const highpass = audioContext.createBiquadFilter();
    highpass.type = 'highpass';
    highpass.frequency.value = 300;
    highpass.Q.value = 0.707;

    const lowpass = audioContext.createBiquadFilter();
    lowpass.type = 'lowpass';
    lowpass.frequency.value = 4000;
    lowpass.Q.value = 0.707;

    // Connect microphone stream -> highpass -> lowpass -> analyser
    const sourceNode = audioContext.createMediaStreamSource(mediaStream);
    sourceNode.connect(highpass);
    highpass.connect(lowpass);
    lowpass.connect(analyser);

    audioContextRef.current = audioContext;
    analyserRef.current = analyser;
    sourceNodeRef.current = sourceNode;
    highpassRef.current = highpass;
    lowpassRef.current = lowpass;

    // Buffer to hold frequency data
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    // ğŸ”§ ä¿®å¤ï¼šåªè®¡ç®—å¸¦é€šèŒƒå›´ï¼ˆ300-4000Hzï¼‰å†…çš„é¢‘ç‡ bin çš„å¹³å‡å€¼
    // ä¹‹å‰çš„ bugï¼šé™¤ä»¥å…¨éƒ¨ 1024 ä¸ª binï¼Œä½†å¸¦é€šæ»¤æ³¢å™¨è®© ~866 ä¸ª bin ä¸º 0ï¼Œ
    // å¯¼è‡´çœŸå®éŸ³é‡è¢«ç¨€é‡Š ~6.5 å€ï¼ˆç”¨æˆ·è¯´è¯éŸ³é‡ 20 â†’ å¹³å‡ååªæœ‰ 3ï¼‰
    const sampleRate = audioContext.sampleRate;
    const freqPerBin = sampleRate / fftSize;
    const lowBin = Math.ceil(300 / freqPerBin);   // 300Hz å¯¹åº”çš„ bin
    const highBin = Math.floor(4000 / freqPerBin); // 4000Hz å¯¹åº”çš„ bin
    const passbandBinCount = highBin - lowBin + 1;

    console.log('ğŸ”Š [VAD] é¢‘ç‡åˆ†æå‚æ•°:', {
      sampleRate,
      freqPerBin: freqPerBin.toFixed(1),
      totalBins: bufferLength,
      passbandBins: `${lowBin}-${highBin} (${passbandBinCount} bins)`,
    });

    const minSpeechDurationMs = minSpeechDuration; // æœ€çŸ­æŒç»­æ—¶é—´ï¼šè¿‡æ»¤çŸ­ä¿ƒå™ªéŸ³
    const risingThreshold = threshold; // ä¸Šå‡é˜ˆå€¼ï¼šè¿›å…¥è¯´è¯çŠ¶æ€
    const fallingThreshold = Math.max(5, threshold - 12); // ä¸‹é™é˜ˆå€¼ï¼šé€€å‡ºè¯´è¯çŠ¶æ€ï¼Œé¿å…æŠ–åŠ¨

    // Voice activity detection loop
    let frameCount = 0;
    const detectVoiceActivity = () => {
      if (!analyserRef.current) return;

      analyserRef.current.getByteFrequencyData(dataArray);

      // åªè®¡ç®—å¸¦é€šèŒƒå›´å†… bin çš„å¹³å‡å€¼ï¼ˆ300-4000Hzï¼‰
      let sum = 0;
      for (let i = lowBin; i <= highBin; i++) {
        sum += dataArray[i];
      }
      const average = sum / passbandBinCount;

      // è¯Šæ–­æ—¥å¿—ï¼šæ¯ 60 å¸§ï¼ˆçº¦ 1 ç§’ï¼‰è¾“å‡ºä¸€æ¬¡ VAD çŠ¶æ€
      frameCount++;
      if (frameCount % 60 === 1) {
        console.log('ğŸ”Š [VAD Loop]', {
          frame: frameCount,
          volume: Math.round(average),
          audioCtxState: audioContext.state,
          threshold: risingThreshold,
          isSpeaking: speakingRef.current,
        });
      }

      // Update current volume for UI display
      setCurrentVolume(Math.round(average));

      const now = Date.now();

      // ä¸Šå‡æ²¿ï¼šè¶…è¿‡ä¸Šå‡é˜ˆå€¼åå¼€å§‹è®¡æ—¶ï¼Œè¶…è¿‡æœ€çŸ­æŒç»­æ—¶é—´æ‰è®¤å®šä¸ºâ€œè¯´è¯â€
      if (average > risingThreshold) {
        if (speechStartRef.current === null) {
          speechStartRef.current = now;
        }

        const duration = now - speechStartRef.current;
        if (!speakingRef.current && duration >= minSpeechDurationMs) {
          setIsSpeaking(true);
          speakingRef.current = true;
          setLastSpeakingTime(now);
        }
      } else if (average < fallingThreshold) {
        // ä¸‹é™æ²¿ï¼šä½äºä¸‹é™é˜ˆå€¼ç«‹å³é‡Šæ”¾è¯´è¯çŠ¶æ€ï¼Œé¿å…çŠ¶æ€ç²˜æ»
        speechStartRef.current = null;
        if (speakingRef.current) {
          setIsSpeaking(false);
          speakingRef.current = false;
        }
      }

      // Continue monitoring
      animationFrameRef.current = requestAnimationFrame(detectVoiceActivity);
    };

    // Start monitoring
    detectVoiceActivity();

    // Cleanup
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (sourceNodeRef.current) {
        sourceNodeRef.current.disconnect();
      }
      if (highpassRef.current) {
        highpassRef.current.disconnect();
      }
      if (lowpassRef.current) {
        lowpassRef.current.disconnect();
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      speakingRef.current = false;
      speechStartRef.current = null;
    };
  }, [enabled, mediaStream, threshold, smoothingTimeConstant, fftSize, minSpeechDuration]);

  return {
    isSpeaking,
    currentVolume,
    lastSpeakingTime: lastSpeakingTime > 0 ? new Date(lastSpeakingTime) : null,
  };
}
