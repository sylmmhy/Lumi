import { useState, useRef, useCallback } from 'react';

/**
 * Voice Input Hook - è¯­éŸ³è¾“å…¥åŠŸèƒ½
 * 
 * èŒè´£ï¼š
 * - å½•åˆ¶ç”¨æˆ·è¯­éŸ³
 * - ç›‘æ§éŸ³é‡ç”Ÿæˆæ³¢å½¢æ•°æ®
 * - å‘é€åˆ°æœåŠ¡å™¨è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
 */

export interface UseVoiceInputOptions {
  /** è½¬å†™æˆåŠŸåçš„å›è°ƒ */
  onTranscript?: (text: string) => void;
  /** é”™è¯¯å›è°ƒ */
  onError?: (error: string) => void;
}

export function useVoiceInput(options: UseVoiceInputOptions = {}) {
  const { onTranscript, onError } = options;

  // çŠ¶æ€
  const [isVoiceMode, setIsVoiceMode] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [waveformHeights, setWaveformHeights] = useState<number[]>([10, 15, 20, 15]);
  const [isTranscribing, setIsTranscribing] = useState(false);

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recordingTimerRef = useRef<NodeJS.Timeout | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  /**
   * ç›‘æ§éŸ³é¢‘ç”µå¹³ï¼Œç”Ÿæˆæ³¢å½¢æ•°æ®
   */
  const monitorAudioLevel = useCallback((stream: MediaStream) => {
    try {
      const audioContext = new AudioContext();
      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.8;
      microphone.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;

      const dataArray = new Uint8Array(analyser.frequencyBinCount);

      const detectLevel = () => {
        if (!analyserRef.current) return;

        analyserRef.current.getByteFrequencyData(dataArray);

        // è®¡ç®—å¹³å‡éŸ³é‡
        const sum = dataArray.reduce((acc, val) => acc + val, 0);
        const average = sum / dataArray.length;

        // å½’ä¸€åŒ–åˆ° 0-1 èŒƒå›´
        const normalized = Math.min((average / 128) * 1.5, 1);

        // æ ¹æ®éŸ³é‡æ›´æ–°æ³¢å½¢é«˜åº¦
        const baseHeight = 10;
        const maxHeight = 40;
        const newHeights = [0.8, 1.0, 0.9, 0.7].map((variance) => {
          const random = Math.random() * 0.5 + 0.5;
          const intensity = normalized * random * variance;
          return baseHeight + (maxHeight - baseHeight) * intensity;
        });

        setWaveformHeights(newHeights);
        animationFrameRef.current = requestAnimationFrame(detectLevel);
      };

      detectLevel();
    } catch (error) {
      console.warn('æ— æ³•åˆå§‹åŒ–éŸ³é¢‘ç›‘æ§:', error);
    }
  }, []);

  /**
   * åœæ­¢éŸ³é¢‘ç›‘æ§
   */
  const stopAudioMonitoring = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    analyserRef.current = null;
    setWaveformHeights([10, 15, 20, 15]);
  }, []);

  /**
   * è½¬å†™éŸ³é¢‘
   */
  const transcribeAudio = useCallback(async () => {
    try {
      if (import.meta.env.DEV) {
        console.log('ğŸ¤ å¼€å§‹éŸ³é¢‘è½¬å†™...');
      }

      setIsTranscribing(true);

      // ä» chunks åˆ›å»º audio blob
      const audioBlob = new Blob(audioChunksRef.current, {
        type: mediaRecorderRef.current?.mimeType || 'audio/webm'
      });

      if (import.meta.env.DEV) {
        console.log('éŸ³é¢‘ blob å¤§å°:', audioBlob.size, 'å­—èŠ‚');
      }

      if (audioBlob.size === 0) {
        throw new Error('æ²¡æœ‰å½•åˆ¶åˆ°éŸ³é¢‘');
      }

      // åˆ›å»º FormData
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');
      formData.append('type', 'final');
      formData.append('timestamp', new Date().toISOString());

      // å‘é€åˆ° Supabase function
      const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
      const supabaseKey = import.meta.env.VITE_SUPABASE_ANON_KEY;

      const response = await fetch(
        `${supabaseUrl}/functions/v1/transcribe-audio`,
        {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${supabaseKey}`
          },
          body: formData
        }
      );

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        throw new Error(errorData.error || `è½¬å†™å¤±è´¥: ${response.status}`);
      }

      const result = await response.json();

      if (result.success) {
        if (import.meta.env.DEV) {
          console.log('âœ… è½¬å†™æˆåŠŸ:', result.transcript);
        }
        onTranscript?.(result.transcript);
      } else {
        throw new Error(result.error || 'è½¬å†™å¤±è´¥');
      }
    } catch (error) {
      console.error('âŒ è½¬å†™é”™è¯¯:', error);
      const errorMessage = error instanceof Error ? error.message : 'è½¬å†™å¤±è´¥';
      onError?.(errorMessage);
      alert('è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚');
    } finally {
      setIsTranscribing(false);
    }
  }, [onTranscript, onError]);

  /**
   * å¼€å§‹å½•éŸ³
   */
  const startRecording = useCallback(async () => {
    try {
      if (import.meta.env.DEV) {
        console.log('ğŸ¤ å¼€å§‹è¯­éŸ³å½•åˆ¶...');
      }

      // è¯·æ±‚éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // åˆ›å»º MediaRecorder
      let mimeType = 'audio/webm';
      if (!MediaRecorder.isTypeSupported('audio/webm')) {
        if (MediaRecorder.isTypeSupported('audio/mp4')) {
          mimeType = 'audio/mp4';
        } else if (MediaRecorder.isTypeSupported('audio/ogg')) {
          mimeType = 'audio/ogg';
        }
      }

      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      // æ”¶é›†éŸ³é¢‘æ•°æ®
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      // å¼€å§‹å½•åˆ¶
      mediaRecorder.start(1000);
      setIsRecording(true);
      setIsVoiceMode(true);
      setRecordingTime(0);

      // å¼€å§‹éŸ³é‡ç›‘æ§
      monitorAudioLevel(stream);

      // å¼€å§‹è®¡æ—¶
      recordingTimerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1);
      }, 1000);

      if (import.meta.env.DEV) {
        console.log('âœ… å½•åˆ¶å·²å¼€å§‹');
      }
    } catch (error) {
      console.error('âŒ å½•åˆ¶å¯åŠ¨é”™è¯¯:', error);
      const errorMessage = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';
      if (errorMessage.includes('Permission denied') || errorMessage.includes('NotAllowedError')) {
        onError?.('éº¦å…‹é£è®¿é—®è¢«æ‹’ç»ï¼Œè¯·åœ¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®ã€‚');
        alert('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®ã€‚');
      } else {
        onError?.(errorMessage);
        alert('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®ã€‚');
      }
    }
  }, [monitorAudioLevel, onError]);

  /**
   * åœæ­¢å½•éŸ³
   */
  const stopRecording = useCallback(async () => {
    if (import.meta.env.DEV) {
      console.log('ğŸ›‘ åœæ­¢è¯­éŸ³å½•åˆ¶...');
    }

    if (!mediaRecorderRef.current) return;

    // åœæ­¢è®¡æ—¶
    if (recordingTimerRef.current) {
      clearInterval(recordingTimerRef.current);
      recordingTimerRef.current = null;
    }

    // åœæ­¢éŸ³é‡ç›‘æ§
    stopAudioMonitoring();

    return new Promise<void>((resolve) => {
      const mediaRecorder = mediaRecorderRef.current!;

      mediaRecorder.onstop = async () => {
        // åœæ­¢éº¦å…‹é£æµ
        mediaRecorder.stream.getTracks().forEach(track => track.stop());

        // è½¬å†™éŸ³é¢‘
        await transcribeAudio();

        setIsRecording(false);
        setIsVoiceMode(false);
        setRecordingTime(0);

        resolve();
      };

      if (mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      } else {
        resolve();
      }
    });
  }, [stopAudioMonitoring, transcribeAudio]);

  /**
   * å–æ¶ˆå½•éŸ³ï¼ˆä¸è½¬å†™ï¼‰
   */
  const cancelRecording = useCallback(() => {
    if (import.meta.env.DEV) {
      console.log('âŒ å–æ¶ˆè¯­éŸ³å½•åˆ¶');
    }

    // åœæ­¢è®¡æ—¶
    if (recordingTimerRef.current) {
      clearInterval(recordingTimerRef.current);
      recordingTimerRef.current = null;
    }

    // åœæ­¢éŸ³é‡ç›‘æ§
    stopAudioMonitoring();

    // åœæ­¢å½•åˆ¶
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
      mediaRecorderRef.current.stop();
    }

    // æ¸…é™¤çŠ¶æ€
    audioChunksRef.current = [];
    setIsRecording(false);
    setIsVoiceMode(false);
    setRecordingTime(0);
  }, [stopAudioMonitoring]);

  return {
    // çŠ¶æ€
    isVoiceMode,
    isRecording,
    recordingTime,
    waveformHeights,
    isTranscribing,

    // æ“ä½œ
    startRecording,
    stopRecording,
    cancelRecording,
  };
}

