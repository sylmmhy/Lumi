import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

// Azure AI Foundry é…ç½® - ä½¿ç”¨ OpenAI å…¼å®¹çš„ REST API
const AZURE_ENDPOINT = Deno.env.get('AZURE_AI_ENDPOINT') || 'https://conta-mcvprtb1-eastus2.openai.azure.com'
const AZURE_API_KEY = Deno.env.get('AZURE_AI_API_KEY')
const MODEL_NAME = Deno.env.get('MEMORY_EXTRACTOR_MODEL') || 'gpt-5.1-chat'
const EMBEDDING_MODEL = Deno.env.get('MEMORY_EMBEDDING_MODEL') || 'text-embedding-3-large'

// è®°å¿†æ•´åˆé…ç½®
const SIMILARITY_THRESHOLD = 0.85  // ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼è§†ä¸ºé‡å¤

// è®°å¿†æå–çš„ç³»ç»Ÿæç¤ºè¯
const EXTRACTION_PROMPT = `You are an AI Coach behavioral pattern extractor. Your job is to identify PATTERNS, PREFERENCES, and SUCCESS RECORDS from user conversations.

## STRICT RULES - MUST FOLLOW

### NEVER EXTRACT (Auto-reject these):
- Time/date mentions ("it's 4pm", "today", "morning")
- Basic intentions ("wants to workout", "going to read")
- Greetings or small talk
- Single events without pattern significance
- What AI said (only extract USER patterns)

### ONLY EXTRACT (High-value insights):

**1. AI INTERACTION PREFERENCES** [Tag: PREF]
User feedback on how AI should communicate.
Examples:
- "User dislikes being rushed or pressured by AI"
- "User prefers gentle encouragement over strict commands"
- "User wants AI to be more direct and less chatty"

**2. PROCRASTINATION TRIGGERS** [Tag: PROC]
WHY user avoids or delays specific tasks.
Examples:
- "User avoids exercise because it feels overwhelming to start"
- "User procrastinates on reading due to fear of not understanding"

**3. PSYCHOSOMATIC PATTERNS** [Tag: SOMA]
Physical symptoms tied to specific activities (possible psychological resistance).
Examples:
- "User reports recurring headaches specifically before workout - possible avoidance pattern"
- "User feels tired only when it's time to study"

**4. EMOTIONAL TRIGGERS** [Tag: EMO]
Emotions consistently tied to specific tasks or situations.
Examples:
- "User feels anxious when facing deadlines"
- "User gets frustrated when interrupted during focus time"

**5. SELF-SABOTAGE PATTERNS** [Tag: SAB]
Recurring behaviors that undermine user's goals.
Examples:
- "User checks phone immediately before important tasks"
- "User makes excuses about time when avoiding exercise"

**6. SUCCESS RECORDS** [Tag: SUCCESS]
Completed tasks - ALWAYS extract when the conversation indicates task completion or the metadata shows task_completed=true.
Examples:
- "User completed 5-minute workout session successfully"
- "User finished brushing teeth task"
- "User overcame initial resistance and completed the full task"

IMPORTANT for SUCCESS - Extract rich details:
- Extract when user completed the task (timer ended, user said "done", "finished", etc.)
- Note if user overcame difficulty during the task (wanted to quit but pushed through)
- Note emotional state at completion if mentioned ("felt proud", "relieved", "happy")
- Note any specific achievements ("did more reps than usual", "finished faster")
- Note if this seemed easier or harder than usual for them
- For SUCCESS tag, include "metadata" field with:
  - duration_minutes: number (if known)
  - overcame_resistance: boolean (true if they struggled but pushed through)
  - completion_mood: string ("proud" | "relieved" | "satisfied" | "neutral" | null)
  - difficulty_perception: string ("easier_than_usual" | "normal" | "harder_than_usual" | null)

## OUTPUT FORMAT

Return a JSON array of extracted memories. Each memory should have:
- "content": The memory text (be specific, include TASK and PATTERN)
- "tag": One of PREF, PROC, SOMA, EMO, SAB, SUCCESS
- "confidence": 0.0-1.0 how confident you are this is a real pattern
- "metadata": (optional, mainly for SUCCESS) {
    "duration_minutes": number,
    "overcame_resistance": boolean,
    "completion_mood": "proud" | "relieved" | "satisfied" | "neutral" | null,
    "difficulty_perception": "easier_than_usual" | "normal" | "harder_than_usual" | null
  }

If there are NO meaningful patterns to extract, return an empty array: []

## EXAMPLES

Input conversation about workout with user saying "Every time when about workout, I feel headache"

Output:
[
  {
    "content": "User experiences recurring headaches specifically when thinking about working out - likely psychological resistance to exercise",
    "tag": "SOMA",
    "confidence": 0.85
  }
]

Input conversation where user completed a 5-minute workout task and said "I did it! That wasn't so bad"

Output:
[
  {
    "content": "User successfully completed workout task and expressed positive surprise - found it easier than expected",
    "tag": "SUCCESS",
    "confidence": 0.95,
    "metadata": { "overcame_resistance": false, "completion_mood": "satisfied", "difficulty_perception": "easier_than_usual" }
  }
]

Input conversation where user struggled but finished, saying "I wanted to quit at minute 2 but I pushed through. I'm so proud of myself!"

Output:
[
  {
    "content": "User completed task despite wanting to quit at the 2-minute mark - showed strong persistence and felt proud of pushing through",
    "tag": "SUCCESS",
    "confidence": 0.95,
    "metadata": { "overcame_resistance": true, "completion_mood": "proud", "difficulty_perception": "harder_than_usual" }
  }
]

Input conversation with user just saying "It's 4pm and I need to work out"

Output:
[]

## IMPORTANT
- Quality over quantity: Only extract MEANINGFUL patterns
- Be specific: Include the TASK and the PATTERN
- Note frequency if mentioned: "always", "every time", "usually"
- Include psychological insight when the pattern suggests it
- For SUCCESS: Always extract when task is completed - this helps with positive reinforcement`

// è®°å¿†åˆå¹¶çš„ç³»ç»Ÿæç¤ºè¯
const MERGE_PROMPT = `You are a memory consolidation expert. Your task is to merge multiple similar memories into ONE concise, comprehensive memory.

## RULES
1. Preserve ALL unique details from each memory
2. Remove redundancy and repetition
3. Keep the merged memory concise but complete
4. Maintain the same tag type
5. Use the highest confidence among the memories being merged

## OUTPUT FORMAT
Return a JSON object:
{
  "content": "The merged memory text",
  "confidence": 0.0-1.0
}

## EXAMPLE

Input memories:
1. "User feels unable to sleep when tasks are unfinished"
2. "User feels strong anxiety about going to bed because tasks feel unfinished"
3. "User becomes anxious whenever thinking about sleep due to incomplete work"

Output:
{
  "content": "User experiences strong anxiety about going to bed when tasks feel unfinished, which prevents them from falling asleep. The thought of incomplete work triggers stress that interferes with winding down.",
  "confidence": 0.9
}`

interface ExtractMemoryRequest {
  action: 'extract'
  userId: string
  messages: Array<{ role: string; content: string }>
  taskDescription?: string
  metadata?: Record<string, unknown>
}

interface SearchMemoryRequest {
  action: 'search'
  userId: string
  query: string
  limit?: number
}

interface GetMemoriesRequest {
  action: 'get'
  userId: string
  limit?: number
}

interface DeleteMemoryRequest {
  action: 'delete'
  memoryId: string
}

interface ConsolidateMemoryRequest {
  action: 'consolidate'
  userId: string
  tag?: string  // å¯é€‰ï¼šåªæ•´åˆç‰¹å®šæ ‡ç­¾çš„è®°å¿†
}

type MemoryRequest = ExtractMemoryRequest | SearchMemoryRequest | GetMemoriesRequest | DeleteMemoryRequest | ConsolidateMemoryRequest

interface ExtractedMemory {
  content: string
  tag: 'PREF' | 'PROC' | 'SOMA' | 'EMO' | 'SAB' | 'SUCCESS'
  confidence: number
  /** SUCCESS æ ‡ç­¾çš„é¢å¤–å…ƒæ•°æ® */
  metadata?: {
    duration_minutes?: number
    overcame_resistance?: boolean
    completion_mood?: 'proud' | 'relieved' | 'satisfied' | 'neutral' | null
    difficulty_perception?: 'easier_than_usual' | 'normal' | 'harder_than_usual' | null
  }
}

/**
 * ä»ä»»åŠ¡æè¿°æ¨æ–­ä»»åŠ¡ç±»å‹
 * ç”¨äº SUCCESS è®°å¿†çš„åˆ†ç±»ï¼Œæ–¹ä¾¿åç»­æŒ‰ç±»å‹æŸ¥è¯¢
 */
function inferTaskType(taskDescription: string): string {
  if (!taskDescription) return 'general'

  const lower = taskDescription.toLowerCase()

  // è¿åŠ¨å¥èº«ç±»
  if (lower.includes('workout') || lower.includes('exercise') || lower.includes('gym') ||
      lower.includes('fitness') || lower.includes('è¿åŠ¨') || lower.includes('å¥èº«') ||
      lower.includes('é”»ç‚¼') || lower.includes('push-up') || lower.includes('pushup')) {
    return 'workout'
  }

  // ç¡çœ ç±»
  if (lower.includes('sleep') || lower.includes('bed') || lower.includes('rest') ||
      lower.includes('nap') || lower.includes('ç¡') || lower.includes('è§‰') ||
      lower.includes('ä¼‘æ¯')) {
    return 'sleep'
  }

  // åˆ·ç‰™/ä¸ªäººå«ç”Ÿç±»
  if (lower.includes('brush') || lower.includes('teeth') || lower.includes('tooth') ||
      lower.includes('shower') || lower.includes('wash') || lower.includes('åˆ·ç‰™') ||
      lower.includes('æ´—') || lower.includes('ç‰™')) {
    return 'hygiene'
  }

  // åšé¥­ç±»
  if (lower.includes('cook') || lower.includes('meal') || lower.includes('food') ||
      lower.includes('dinner') || lower.includes('lunch') || lower.includes('breakfast') ||
      lower.includes('åšé¥­') || lower.includes('çƒ¹é¥ª') || lower.includes('é¥­')) {
    return 'cooking'
  }

  // æ¸…æ´ç±»
  if (lower.includes('clean') || lower.includes('tidy') || lower.includes('organize') ||
      lower.includes('æ‰“æ‰«') || lower.includes('æ¸…æ´') || lower.includes('æ•´ç†')) {
    return 'cleaning'
  }

  // å­¦ä¹ ç±»
  if (lower.includes('study') || lower.includes('learn') || lower.includes('read') ||
      lower.includes('homework') || lower.includes('å­¦ä¹ ') || lower.includes('è¯»ä¹¦') ||
      lower.includes('ä½œä¸š') || lower.includes('çœ‹ä¹¦')) {
    return 'study'
  }

  // å·¥ä½œç±»
  if (lower.includes('work') || lower.includes('task') || lower.includes('project') ||
      lower.includes('email') || lower.includes('å·¥ä½œ') || lower.includes('ä»»åŠ¡') ||
      lower.includes('é¡¹ç›®')) {
    return 'work'
  }

  // å†¥æƒ³/æ”¾æ¾ç±»
  if (lower.includes('meditat') || lower.includes('breath') || lower.includes('relax') ||
      lower.includes('calm') || lower.includes('å†¥æƒ³') || lower.includes('å‘¼å¸') ||
      lower.includes('æ”¾æ¾')) {
    return 'meditation'
  }

  return 'general'
}

interface ExistingMemory {
  id: string
  content: string
  tag: string
  confidence: number
  similarity: number
}

interface SaveResult {
  action: 'created' | 'merged' | 'skipped'
  memoryId: string
  content: string
  mergedFrom?: string[]
}

/**
 * ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡
 */
async function generateEmbedding(text: string): Promise<number[]> {
  if (!AZURE_API_KEY) {
    throw new Error('AZURE_AI_API_KEY environment variable not set')
  }

  const apiUrl = `${AZURE_ENDPOINT}/openai/v1/embeddings`

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AZURE_API_KEY}`,
    },
    body: JSON.stringify({
      model: EMBEDDING_MODEL,
      input: text,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('Embedding API error:', error)
    throw new Error(`Embedding request failed: ${response.status}`)
  }

  const result = await response.json()
  return result.data?.[0]?.embedding || []
}

/**
 * æŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰è®°å¿†
 */
async function findSimilarMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  embedding: number[],
  tag: string
): Promise<ExistingMemory[]> {
  // ä½¿ç”¨æ•°æ®åº“å‡½æ•°è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
  const { data, error } = await supabase.rpc('search_similar_memories', {
    p_user_id: userId,
    p_embedding: JSON.stringify(embedding),
    p_tag: tag,
    p_threshold: SIMILARITY_THRESHOLD,
    p_limit: 5,
  })

  if (error) {
    console.error('Similar memory search error:', error)
    // å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼ˆè¿ç§»æœªè¿è¡Œï¼‰ï¼Œè¿”å›ç©ºæ•°ç»„
    return []
  }

  return data || []
}

/**
 * ä½¿ç”¨ LLM åˆå¹¶å¤šæ¡ç›¸ä¼¼è®°å¿†
 */
async function mergeMemoriesWithAI(
  memories: Array<{ content: string; confidence: number }>
): Promise<{ content: string; confidence: number }> {
  if (!AZURE_API_KEY) {
    throw new Error('AZURE_AI_API_KEY environment variable not set')
  }

  const memoriesText = memories
    .map((m, i) => `${i + 1}. "${m.content}"`)
    .join('\n')

  const apiUrl = `${AZURE_ENDPOINT}/openai/v1/chat/completions`

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AZURE_API_KEY}`,
    },
    body: JSON.stringify({
      model: MODEL_NAME,
      messages: [
        { role: 'system', content: MERGE_PROMPT },
        { role: 'user', content: `Merge these memories:\n${memoriesText}` }
      ],
      max_completion_tokens: 500,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('Merge API error:', error)
    throw new Error(`Merge request failed: ${response.status}`)
  }

  const result = await response.json()
  const content = result.choices?.[0]?.message?.content

  try {
    const parsed = JSON.parse(content)
    return {
      content: parsed.content,
      confidence: Math.max(...memories.map(m => m.confidence), parsed.confidence || 0.8),
    }
  } catch (e) {
    console.error('Failed to parse merge response:', content, e)
    // å›é€€ï¼šä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„é‚£æ¡
    const best = memories.reduce((a, b) => a.confidence > b.confidence ? a : b)
    return best
  }
}

/**
 * è°ƒç”¨ Azure AI æå–è®°å¿†
 */
async function extractMemoriesWithAI(
  messages: Array<{ role: string; content: string }>,
  taskDescription?: string
): Promise<ExtractedMemory[]> {
  if (!AZURE_API_KEY) {
    throw new Error('AZURE_AI_API_KEY environment variable not set')
  }

  // è¿‡æ»¤æ‰ç©ºæ¶ˆæ¯æˆ–æ— æ•ˆæ¶ˆæ¯
  const validMessages = messages.filter(m => m && m.content && typeof m.content === 'string' && m.content.trim())

  console.log(`Received ${messages.length} messages, ${validMessages.length} valid`)

  if (validMessages.length === 0) {
    console.log('No valid messages to process')
    return []
  }

  // åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯ï¼ˆå› ä¸ºæµå¼è¾“å‡ºä¼šæŠŠä¸€å¥è¯åˆ†æˆå¤šæ¡æ¶ˆæ¯ï¼‰
  const mergedMessages: Array<{ role: string; content: string }> = []
  for (const msg of validMessages) {
    const lastMsg = mergedMessages[mergedMessages.length - 1]
    if (lastMsg && lastMsg.role === msg.role) {
      // åˆå¹¶åˆ°ä¸Šä¸€æ¡æ¶ˆæ¯
      lastMsg.content += msg.content
    } else {
      // æ–°è§’è‰²ï¼Œåˆ›å»ºæ–°æ¶ˆæ¯
      mergedMessages.push({ role: msg.role, content: msg.content })
    }
  }

  console.log(`Merged to ${mergedMessages.length} messages`)

  // æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ŒåŒ…å«å¯¹è¯å†…å®¹
  const conversationText = mergedMessages
    .map(m => `${m.role?.toUpperCase() || 'UNKNOWN'}: ${m.content}`)
    .join('\n')

  console.log(`Conversation text length: ${conversationText.length} chars`)

  const userPrompt = taskDescription
    ? `Task context: "${taskDescription}"\n\nConversation:\n${conversationText}`
    : `Conversation:\n${conversationText}`

  // è°ƒç”¨ Azure AI Foundry - ä½¿ç”¨ OpenAI å…¼å®¹çš„ REST API
  // URL: {endpoint}/openai/v1/chat/completions
  // è®¤è¯: Authorization: Bearer {api_key}
  const apiUrl = `${AZURE_ENDPOINT}/openai/v1/chat/completions`

  console.log(`Calling Azure AI: ${apiUrl} with model: ${MODEL_NAME}`)

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AZURE_API_KEY}`,
    },
    body: JSON.stringify({
      model: MODEL_NAME,  // éœ€è¦åœ¨ body ä¸­æŒ‡å®š model
      messages: [
        { role: 'system', content: EXTRACTION_PROMPT },
        { role: 'user', content: userPrompt }
      ],
      max_completion_tokens: 1000,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('Azure AI error:', error)
    throw new Error(`Azure AI request failed: ${response.status} ${error}`)
  }

  const result = await response.json()
  const content = result.choices?.[0]?.message?.content

  if (!content) {
    console.log('No content in AI response')
    return []
  }

  try {
    const parsed = JSON.parse(content)
    // å¤„ç†å¯èƒ½çš„ä¸åŒæ ¼å¼
    const memories = Array.isArray(parsed) ? parsed : (parsed.memories || parsed.results || [])
    return memories.filter((m: ExtractedMemory) => m.content && m.tag && m.confidence >= 0.5)
  } catch (e) {
    console.error('Failed to parse AI response:', content, e)
    return []
  }
}

/**
 * ä¿å­˜æˆ–åˆå¹¶è®°å¿†åˆ° Supabaseï¼ˆå« Update Phase é€»è¾‘ï¼‰
 *
 * æµç¨‹ï¼š
 * 1. ä¸ºæ¯æ¡æ–°è®°å¿†ç”Ÿæˆ embedding
 * 2. æŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰è®°å¿†
 * 3. å¦‚æœæ‰¾åˆ°ç›¸ä¼¼è®°å¿† â†’ åˆå¹¶å¹¶æ›´æ–°
 * 4. å¦‚æœæ²¡æœ‰ç›¸ä¼¼è®°å¿† â†’ åˆ›å»ºæ–°è®°å¿†
 */
async function saveOrMergeMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  memories: ExtractedMemory[],
  taskDescription?: string,
  metadata?: Record<string, unknown>
): Promise<{ saved: number; merged: number; results: SaveResult[] }> {
  if (memories.length === 0) {
    return { saved: 0, merged: 0, results: [] }
  }

  const results: SaveResult[] = []
  let savedCount = 0
  let mergedCount = 0

  for (const memory of memories) {
    try {
      // ============================================================
      // SUCCESS ç±»å‹ç‰¹æ®Šå¤„ç†ï¼šä¸åˆå¹¶ï¼Œæ¯æ¬¡éƒ½åˆ›å»ºæ–°è®°å½•
      // ============================================================
      if (memory.tag === 'SUCCESS') {
        console.log(`Processing SUCCESS memory for task: ${taskDescription}`)

        // æ¨æ–­ä»»åŠ¡ç±»å‹
        const taskType = inferTaskType(taskDescription || '')

        // è®¡ç®—å½“å‰è¿èƒœï¼ˆåœ¨æ–°è®°å½•ä¹‹å‰ï¼‰
        let currentStreak = 0
        try {
          const { data: streakData } = await supabase.rpc('calculate_user_streak', {
            p_user_id: userId,
            p_task_type: taskType
          })
          currentStreak = streakData || 0
        } catch (e) {
          console.warn('Failed to calculate streak, defaulting to 0:', e)
        }

        // ä»è¯·æ±‚çš„ metadata ä¸­è·å–å®é™…æ—¶é•¿
        const actualDuration = (metadata as Record<string, unknown>)?.actual_duration_minutes as number | undefined
        const thisDuration = memory.metadata?.duration_minutes || actualDuration || null

        // æŸ¥è¯¢è¯¥ä»»åŠ¡ç±»å‹çš„å†å²æœ€ä½³æ—¶é•¿
        let personalBest: number | null = null
        let isNewPersonalBest = false
        try {
          const { data: bestData } = await supabase
            .from('user_memories')
            .select('metadata')
            .eq('user_id', userId)
            .eq('tag', 'SUCCESS')
            .not('metadata->duration_minutes', 'is', null)
            .order('metadata->duration_minutes', { ascending: false })
            .limit(1)

          if (bestData && bestData.length > 0) {
            const bestMetadata = bestData[0].metadata as Record<string, unknown>
            personalBest = (bestMetadata?.duration_minutes as number) || null
          }

          // åˆ¤æ–­æ˜¯å¦åˆ›é€ æ–°çš„ä¸ªäººæœ€ä½³
          if (thisDuration && (!personalBest || thisDuration > personalBest)) {
            isNewPersonalBest = true
            personalBest = thisDuration
            console.log(`ğŸ† New personal best: ${thisDuration} minutes!`)
          }
        } catch (e) {
          console.warn('Failed to check personal best:', e)
        }

        // æ„å»º SUCCESS è®°å¿†çš„å®Œæ•´ metadata
        const successMetadata = {
          // åŸºç¡€å…ƒæ•°æ®
          task_type: taskType,
          completion_date: new Date().toISOString().split('T')[0],
          streak_count: currentStreak + 1, // æ–°çš„è¿èƒœæ•°
          // AI æå–çš„å…ƒæ•°æ®
          duration_minutes: thisDuration,
          overcame_resistance: memory.metadata?.overcame_resistance || false,
          completion_mood: memory.metadata?.completion_mood || null,
          difficulty_perception: memory.metadata?.difficulty_perception || null,
          // ä¸ªäººæœ€ä½³è¿½è¸ª
          is_personal_best: isNewPersonalBest,
          personal_best_at_time: personalBest,
          // è¯·æ±‚å¸¦æ¥çš„å…¶ä»–å…ƒæ•°æ®
          source: (metadata as Record<string, unknown>)?.source || 'ai_coach_session',
          extractedAt: new Date().toISOString(),
        }

        console.log(`SUCCESS metadata:`, successMetadata)

        // ç”Ÿæˆ embeddingï¼ˆå¯é€‰ï¼ŒSUCCESS ä¸éœ€è¦å»é‡ä½†å¯ç”¨äºè¯­ä¹‰æ£€ç´¢ï¼‰
        let embedding: number[] = []
        try {
          embedding = await generateEmbedding(memory.content)
        } catch (e) {
          console.warn('Failed to generate embedding for SUCCESS memory:', e)
        }

        // ç›´æ¥æ’å…¥ï¼Œä¸åšåˆå¹¶
        const insertData: Record<string, unknown> = {
          user_id: userId,
          content: memory.content,
          tag: 'SUCCESS',
          confidence: memory.confidence,
          task_name: taskDescription || null,
          metadata: successMetadata,
        }
        if (embedding.length > 0) {
          insertData.embedding = JSON.stringify(embedding)
        }

        const { data, error } = await supabase
          .from('user_memories')
          .insert(insertData)
          .select()
          .single()

        if (error) {
          console.error('Failed to save SUCCESS memory:', error)
          continue
        }

        console.log(`âœ… SUCCESS memory saved! Streak: ${successMetadata.streak_count}`)
        results.push({ action: 'created', memoryId: data.id, content: memory.content })
        savedCount++
        continue
      }

      // ============================================================
      // å…¶ä»–ç±»å‹ï¼šæ­£å¸¸çš„å»é‡åˆå¹¶é€»è¾‘
      // ============================================================

      // 1. ç”Ÿæˆ embedding
      console.log(`Generating embedding for: ${memory.content.substring(0, 50)}...`)
      const embedding = await generateEmbedding(memory.content)

      if (embedding.length === 0) {
        console.warn('Failed to generate embedding, saving without dedup')
        // å›é€€åˆ°ç®€å•æ’å…¥
        const { data } = await supabase
          .from('user_memories')
          .insert({
            user_id: userId,
            content: memory.content,
            tag: memory.tag,
            confidence: memory.confidence,
            task_name: taskDescription || null,
            metadata: metadata || {},
          })
          .select()
          .single()

        if (data) {
          results.push({ action: 'created', memoryId: data.id, content: memory.content })
          savedCount++
        }
        continue
      }

      // 2. æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†
      const similarMemories = await findSimilarMemories(supabase, userId, embedding, memory.tag)
      console.log(`Found ${similarMemories.length} similar memories for tag ${memory.tag}`)

      if (similarMemories.length > 0) {
        // 3. æœ‰ç›¸ä¼¼è®°å¿† â†’ åˆå¹¶
        const allMemories = [
          { content: memory.content, confidence: memory.confidence },
          ...similarMemories.map(m => ({ content: m.content, confidence: m.confidence }))
        ]

        console.log(`Merging ${allMemories.length} memories...`)
        const merged = await mergeMemoriesWithAI(allMemories)

        // ç”Ÿæˆåˆå¹¶åå†…å®¹çš„æ–° embedding
        const mergedEmbedding = await generateEmbedding(merged.content)

        // æ›´æ–°æœ€ç›¸ä¼¼çš„é‚£æ¡è®°å¿†ï¼ˆä¿ç•™å…¶ IDï¼‰
        const targetMemory = similarMemories[0]
        const mergedFromIds = similarMemories.map(m => m.id)

        const { data, error } = await supabase
          .from('user_memories')
          .update({
            content: merged.content,
            confidence: merged.confidence,
            embedding: JSON.stringify(mergedEmbedding),
            task_name: taskDescription || null,
            merged_from: mergedFromIds,
            metadata: {
              ...metadata,
              lastMergedAt: new Date().toISOString(),
              mergeCount: (similarMemories.length + 1),
            },
          })
          .eq('id', targetMemory.id)
          .select()
          .single()

        if (error) {
          console.error('Failed to update merged memory:', error)
          continue
        }

        // åˆ é™¤å…¶ä»–è¢«åˆå¹¶çš„è®°å¿†ï¼ˆé™¤äº†ç›®æ ‡è®°å¿†ï¼‰
        if (similarMemories.length > 1) {
          const idsToDelete = similarMemories.slice(1).map(m => m.id)
          await supabase
            .from('user_memories')
            .delete()
            .in('id', idsToDelete)
          console.log(`Deleted ${idsToDelete.length} merged source memories`)
        }

        results.push({
          action: 'merged',
          memoryId: targetMemory.id,
          content: merged.content,
          mergedFrom: mergedFromIds,
        })
        mergedCount++

      } else {
        // 4. æ²¡æœ‰ç›¸ä¼¼è®°å¿† â†’ åˆ›å»ºæ–°è®°å¿†
        const { data, error } = await supabase
          .from('user_memories')
          .insert({
            user_id: userId,
            content: memory.content,
            tag: memory.tag,
            confidence: memory.confidence,
            embedding: JSON.stringify(embedding),
            task_name: taskDescription || null,
            metadata: metadata || {},
          })
          .select()
          .single()

        if (error) {
          console.error('Failed to save new memory:', error)
          continue
        }

        results.push({ action: 'created', memoryId: data.id, content: memory.content })
        savedCount++
      }

    } catch (err) {
      console.error(`Error processing memory: ${memory.content.substring(0, 50)}...`, err)
    }
  }

  return { saved: savedCount, merged: mergedCount, results }
}

/**
 * æœç´¢ç›¸å…³è®°å¿†
 */
async function searchMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  query: string,
  limit = 10
) {
  // ç®€å•çš„æ–‡æœ¬æœç´¢ï¼Œåç»­å¯ä»¥æ”¹æˆå‘é‡æœç´¢
  const { data, error } = await supabase
    .from('user_memories')
    .select('*')
    .eq('user_id', userId)
    .ilike('content', `%${query}%`)
    .order('created_at', { ascending: false })
    .limit(limit)

  if (error) {
    throw new Error(`Failed to search memories: ${error.message}`)
  }

  return data || []
}

/**
 * è·å–ç”¨æˆ·æ‰€æœ‰è®°å¿†
 */
async function getMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  limit = 50
) {
  const { data, error } = await supabase
    .from('user_memories')
    .select('*')
    .eq('user_id', userId)
    .order('created_at', { ascending: false })
    .limit(limit)

  if (error) {
    throw new Error(`Failed to get memories: ${error.message}`)
  }

  return data || []
}

/**
 * åˆ é™¤è®°å¿†
 */
async function deleteMemory(
  supabase: ReturnType<typeof createClient>,
  memoryId: string
) {
  const { error } = await supabase
    .from('user_memories')
    .delete()
    .eq('id', memoryId)

  if (error) {
    throw new Error(`Failed to delete memory: ${error.message}`)
  }

  return { success: true, memoryId }
}

/**
 * æ•´åˆç°æœ‰è®°å¿†ï¼ˆæ¸…ç†é‡å¤ï¼‰
 * å¯¹ç”¨æˆ·çš„æ¯ä¸ªæ ‡ç­¾ç±»åˆ«ï¼Œæ‰¾å‡ºç›¸ä¼¼è®°å¿†å¹¶åˆå¹¶
 */
async function consolidateMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  targetTag?: string
): Promise<{ processed: number; merged: number; deleted: number }> {
  const tags = targetTag ? [targetTag] : ['PREF', 'PROC', 'SOMA', 'EMO', 'SAB']

  let totalProcessed = 0
  let totalMerged = 0
  let totalDeleted = 0

  // æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆæ¯” embedding é˜ˆå€¼ä½ä¸€äº›ï¼Œå› ä¸ºå‡†ç¡®æ€§è¾ƒä½ï¼‰
  const TEXT_SIMILARITY_THRESHOLD = 0.4

  for (const tag of tags) {
    // è·å–è¯¥æ ‡ç­¾çš„æ‰€æœ‰è®°å¿†
    console.log(`Querying memories for user ${userId}, tag ${tag}...`)

    const { data: memories, error } = await supabase
      .from('user_memories')
      .select('*')
      .eq('user_id', userId)
      .eq('tag', tag)
      .order('created_at', { ascending: true })

    if (error) {
      console.error(`Query error for tag ${tag}:`, error)
      continue
    }

    if (!memories || memories.length === 0) {
      console.log(`No memories found for tag ${tag}`)
      continue
    }

    if (memories.length < 2) {
      console.log(`Only ${memories.length} memory for tag ${tag}, skipping (need at least 2 to consolidate)`)
      totalProcessed += memories.length
      continue
    }

    console.log(`Processing ${memories.length} memories for tag ${tag}`)
    totalProcessed += memories.length

    // å°è¯•ä¸ºæ²¡æœ‰ embedding çš„è®°å¿†ç”Ÿæˆ embeddingï¼ˆä½†å¤±è´¥ä¸é˜»å¡æµç¨‹ï¼‰
    let embeddingAvailable = false
    for (const memory of memories) {
      if (!memory.embedding) {
        try {
          const embedding = await generateEmbedding(memory.content)
          if (embedding && embedding.length > 0) {
            await supabase
              .from('user_memories')
              .update({ embedding: JSON.stringify(embedding) })
              .eq('id', memory.id)
            memory.embedding = embedding
            embeddingAvailable = true
          }
        } catch (err) {
          console.warn(`Embedding generation failed for ${memory.id}, will use text similarity:`, err)
        }
      } else {
        embeddingAvailable = true
      }
    }

    console.log(`Embedding available: ${embeddingAvailable}, using ${embeddingAvailable ? 'vector' : 'text'} similarity`)

    // æ‰¾å‡ºç›¸ä¼¼ç»„ï¼ˆä½¿ç”¨ç®€å•çš„è´ªå¿ƒèšç±»ï¼‰
    const processed = new Set<string>()
    const groups: typeof memories[] = []

    for (const memory of memories) {
      if (processed.has(memory.id)) continue

      const group = [memory]
      processed.add(memory.id)

      // æ‰¾å‡ºä¸å½“å‰è®°å¿†ç›¸ä¼¼çš„å…¶ä»–è®°å¿†
      for (const other of memories) {
        if (processed.has(other.id)) continue

        let similarity = 0

        // ä¼˜å…ˆä½¿ç”¨ embedding ç›¸ä¼¼åº¦
        if (memory.embedding && other.embedding) {
          try {
            similarity = cosineSimilarity(
              typeof memory.embedding === 'string' ? JSON.parse(memory.embedding) : memory.embedding,
              typeof other.embedding === 'string' ? JSON.parse(other.embedding) : other.embedding
            )
            if (similarity >= SIMILARITY_THRESHOLD) {
              group.push(other)
              processed.add(other.id)
              console.log(`  Embedding match: "${memory.content.substring(0, 30)}..." ~ "${other.content.substring(0, 30)}..." (${(similarity * 100).toFixed(1)}%)`)
            }
          } catch (err) {
            console.warn('Failed to parse embeddings, falling back to text similarity')
            similarity = textSimilarity(memory.content, other.content)
            if (similarity >= TEXT_SIMILARITY_THRESHOLD) {
              group.push(other)
              processed.add(other.id)
              console.log(`  Text match: "${memory.content.substring(0, 30)}..." ~ "${other.content.substring(0, 30)}..." (${(similarity * 100).toFixed(1)}%)`)
            }
          }
        } else {
          // å›é€€åˆ°æ–‡æœ¬ç›¸ä¼¼åº¦
          similarity = textSimilarity(memory.content, other.content)
          if (similarity >= TEXT_SIMILARITY_THRESHOLD) {
            group.push(other)
            processed.add(other.id)
            console.log(`  Text match: "${memory.content.substring(0, 30)}..." ~ "${other.content.substring(0, 30)}..." (${(similarity * 100).toFixed(1)}%)`)
          }
        }
      }

      if (group.length > 1) {
        groups.push(group)
      }
    }

    console.log(`Found ${groups.length} groups of similar memories for tag ${tag}`)

    // åˆå¹¶æ¯ä¸ªç›¸ä¼¼ç»„
    for (const group of groups) {
      try {
        // ç”¨ LLM åˆå¹¶
        const merged = await mergeMemoriesWithAI(
          group.map(m => ({ content: m.content, confidence: m.confidence }))
        )

        // å°è¯•ç”Ÿæˆæ–°çš„ embeddingï¼ˆå¯é€‰ï¼Œå¤±è´¥ä¸é˜»å¡ï¼‰
        let mergedEmbedding: number[] | null = null
        try {
          mergedEmbedding = await generateEmbedding(merged.content)
          if (!mergedEmbedding || mergedEmbedding.length === 0) {
            mergedEmbedding = null
          }
        } catch (embeddingErr) {
          console.warn('Failed to generate embedding for merged content, proceeding without:', embeddingErr)
        }

        // æ›´æ–°ç¬¬ä¸€æ¡è®°å¿†
        const targetId = group[0].id
        const mergedFromIds = group.map(m => m.id)

        // æ„å»ºæ›´æ–°å¯¹è±¡ï¼ˆembedding å¯é€‰ï¼‰
        const updateData: Record<string, unknown> = {
          content: merged.content,
          confidence: merged.confidence,
          merged_from: mergedFromIds,
          metadata: {
            consolidatedAt: new Date().toISOString(),
            mergeCount: group.length,
          },
        }
        if (mergedEmbedding) {
          updateData.embedding = JSON.stringify(mergedEmbedding)
        }

        await supabase
          .from('user_memories')
          .update(updateData)
          .eq('id', targetId)

        // åˆ é™¤å…¶ä»–è®°å¿†
        const idsToDelete = group.slice(1).map(m => m.id)
        if (idsToDelete.length > 0) {
          await supabase
            .from('user_memories')
            .delete()
            .in('id', idsToDelete)

          totalDeleted += idsToDelete.length
        }

        totalMerged++
        console.log(`Merged ${group.length} memories into one (tag: ${tag})`)

      } catch (err) {
        console.error(`Failed to merge group:`, err)
      }
    }
  }

  return { processed: totalProcessed, merged: totalMerged, deleted: totalDeleted }
}

/**
 * è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
 */
function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) return 0

  let dotProduct = 0
  let normA = 0
  let normB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    normA += a[i] * a[i]
    normB += b[i] * b[i]
  }

  if (normA === 0 || normB === 0) return 0
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
}

/**
 * è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ (Jaccard + å…³é”®è¯é‡å )
 * ç”¨äºåœ¨æ²¡æœ‰ embedding æ—¶ä½œä¸ºå›é€€æ–¹æ¡ˆ
 */
function textSimilarity(text1: string, text2: string): number {
  // ç®€å•åˆ†è¯
  const tokenize = (text: string) => {
    return text.toLowerCase()
      .replace(/[^\w\s]/g, ' ')
      .split(/\s+/)
      .filter(t => t.length > 2)
  }

  const tokens1 = new Set(tokenize(text1))
  const tokens2 = new Set(tokenize(text2))

  // Jaccard ç›¸ä¼¼åº¦
  const intersection = [...tokens1].filter(t => tokens2.has(t)).length
  const union = new Set([...tokens1, ...tokens2]).size

  if (union === 0) return 0
  return intersection / union
}

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    // åˆå§‹åŒ– Supabase client
    const supabaseUrl = Deno.env.get('SUPABASE_URL')!
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    const supabase = createClient(supabaseUrl, supabaseServiceKey)

    const body: MemoryRequest = await req.json()

    let result: unknown

    switch (body.action) {
      case 'extract': {
        const { userId, messages, taskDescription, metadata } = body as ExtractMemoryRequest
        if (!userId || !messages || messages.length === 0) {
          throw new Error('Missing required fields: userId, messages')
        }

        console.log(`Extracting memories for user: ${userId}, messages: ${messages.length}`)
        console.log(`Task completed: ${(metadata as Record<string, unknown>)?.task_completed}, duration: ${(metadata as Record<string, unknown>)?.actual_duration_minutes} min`)

        // 1. ç”¨ AI æå–è®°å¿†
        const extractedMemories = await extractMemoriesWithAI(messages, taskDescription)
        console.log(`Extracted ${extractedMemories.length} memories`)

        // 2. å¦‚æœä»»åŠ¡å®Œæˆäº†ï¼Œç¡®ä¿æœ‰ä¸€æ¡ SUCCESS è®°å¿†
        const taskCompleted = (metadata as Record<string, unknown>)?.task_completed === true
        const hasSuccessMemory = extractedMemories.some(m => m.tag === 'SUCCESS')

        if (taskCompleted && !hasSuccessMemory) {
          console.log('Task completed but no SUCCESS memory extracted, adding one automatically')
          const actualDuration = (metadata as Record<string, unknown>)?.actual_duration_minutes as number | undefined
          const taskType = inferTaskType(taskDescription || '')

          extractedMemories.push({
            content: `User successfully completed ${taskType} task: "${taskDescription}"${actualDuration ? ` (${actualDuration} minutes)` : ''}`,
            tag: 'SUCCESS',
            confidence: 0.95,
            metadata: {
              duration_minutes: actualDuration,
              overcame_resistance: false, // é»˜è®¤ï¼Œé™¤é AI æ£€æµ‹åˆ°
            }
          })
          console.log('Auto-added SUCCESS memory')
        }

        // 3. ä¿å­˜æˆ–åˆå¹¶åˆ° Supabase (Update Phase)
        if (extractedMemories.length > 0) {
          const saveResult = await saveOrMergeMemories(supabase, userId, extractedMemories, taskDescription, {
            ...metadata,
            taskDescription,
            extractedAt: new Date().toISOString(),
          })
          result = {
            extracted: extractedMemories.length,
            saved: saveResult.saved,
            merged: saveResult.merged,
            results: saveResult.results,
            memories: extractedMemories
          }
        } else {
          result = { extracted: 0, saved: 0, merged: 0, results: [], memories: [] }
        }
        break
      }

      case 'search': {
        const { userId, query, limit } = body as SearchMemoryRequest
        if (!userId || !query) {
          throw new Error('Missing required fields: userId, query')
        }
        console.log(`Searching memories for user: ${userId}, query: ${query}`)
        result = await searchMemories(supabase, userId, query, limit)
        break
      }

      case 'get': {
        const { userId, limit } = body as GetMemoriesRequest
        if (!userId) {
          throw new Error('Missing required field: userId')
        }
        console.log(`Getting memories for user: ${userId}`)
        result = await getMemories(supabase, userId, limit)
        break
      }

      case 'delete': {
        const { memoryId } = body as DeleteMemoryRequest
        if (!memoryId) {
          throw new Error('Missing required field: memoryId')
        }
        console.log(`Deleting memory: ${memoryId}`)
        result = await deleteMemory(supabase, memoryId)
        break
      }

      case 'consolidate': {
        const { userId, tag } = body as ConsolidateMemoryRequest
        if (!userId) {
          throw new Error('Missing required field: userId')
        }
        console.log(`=== CONSOLIDATE REQUEST ===`)
        console.log(`userId: ${userId}`)
        console.log(`userId type: ${typeof userId}`)
        console.log(`tag filter: ${tag || 'all'}`)

        // å…ˆéªŒè¯ç”¨æˆ·æ˜¯å¦æœ‰è®°å¿†
        const { data: checkData, error: checkError } = await supabase
          .from('user_memories')
          .select('id, tag')
          .eq('user_id', userId)
          .limit(10)

        console.log(`Pre-check: found ${checkData?.length || 0} memories for this user`)
        if (checkError) {
          console.error('Pre-check error:', checkError)
        }
        if (checkData && checkData.length > 0) {
          console.log('Sample memories:', checkData.slice(0, 3))
        }

        result = await consolidateMemories(supabase, userId, tag)
        console.log(`Consolidate result:`, result)
        break
      }

      default:
        throw new Error(`Unknown action: ${(body as { action: string }).action}`)
    }

    return new Response(
      JSON.stringify(result),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )

  } catch (error) {
    console.error('Memory extractor error:', error)
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  }
})
