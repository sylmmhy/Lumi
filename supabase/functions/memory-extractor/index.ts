import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

// Azure AI Foundry é…ç½® - ä½¿ç”¨ OpenAI å…¼å®¹çš„ REST API
const AZURE_ENDPOINT = Deno.env.get('AZURE_AI_ENDPOINT') || 'https://conta-mcvprtb1-eastus2.openai.azure.com'
const AZURE_API_KEY = Deno.env.get('AZURE_AI_API_KEY')
const MODEL_NAME = Deno.env.get('MEMORY_EXTRACTOR_MODEL') || 'gpt-5.1-chat'

// Embedding é…ç½® - å¯ä»¥ä½¿ç”¨ç‹¬ç«‹çš„ endpoint å’Œ API key
// å¦‚æœæœªè®¾ç½®ï¼Œåˆ™å›é€€åˆ°ä¸» Azure é…ç½®
const EMBEDDING_ENDPOINT = Deno.env.get('AZURE_EMBEDDING_ENDPOINT') || AZURE_ENDPOINT
const EMBEDDING_API_KEY = Deno.env.get('AZURE_EMBEDDING_API_KEY') || AZURE_API_KEY
const EMBEDDING_MODEL = Deno.env.get('MEMORY_EMBEDDING_MODEL') || 'text-embedding-3-small'

// è®°å¿†æ•´åˆé…ç½®
const SIMILARITY_THRESHOLD = 0.85  // ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé«˜äºæ­¤å€¼è§†ä¸ºé‡å¤

/**
 * è®¡ç®—è®°å¿†çš„åˆå§‹é‡è¦æ€§è¯„åˆ†
 * åŸºäºæ ‡ç­¾ç±»å‹ã€ç½®ä¿¡åº¦å’Œå†…å®¹ç‰¹å¾
 *
 * è¯„åˆ†è§„åˆ™ï¼š
 * - PREFï¼ˆåå¥½ï¼‰: åŸºç¡€ 0.7ï¼Œç”¨æˆ·åå¥½é€šå¸¸å¾ˆé‡è¦
 * - EFFECTIVEï¼ˆæœ‰æ•ˆæ¿€åŠ±ï¼‰: åŸºç¡€ 0.8ï¼ŒæˆåŠŸçš„æ¿€åŠ±æ–¹å¼éå¸¸é‡è¦
 * - PROC/EMO/SAB: åŸºç¡€ 0.5ï¼Œè¡Œä¸ºæ¨¡å¼è®°å¿†
 * - SOMA: åŸºç¡€ 0.4ï¼Œèº«å¿ƒååº”ç›¸å¯¹æ¬¡è¦
 *
 * è°ƒæ•´å› ç´ ï¼š
 * - ç½®ä¿¡åº¦é«˜äº 0.8 â†’ +0.1
 * - å†…å®¹åŒ…å«å…·ä½“ç»†èŠ‚ï¼ˆæ•°å­—ã€åè¯ï¼‰â†’ +0.1
 * - å†…å®¹è¾ƒé•¿ï¼ˆè¶…è¿‡ 100 å­—ç¬¦ï¼‰â†’ +0.05
 */
function calculateImportanceScore(memory: ExtractedMemory): number {
  // åŸºç¡€åˆ†æ•°ï¼ˆæŒ‰æ ‡ç­¾ç±»å‹ï¼‰
  const baseScores: Record<string, number> = {
    'PREF': 0.7,       // AI äº¤äº’åå¥½
    'EFFECTIVE': 0.8,  // æœ‰æ•ˆæ¿€åŠ±æ–¹å¼ï¼ˆæœ€é‡è¦ï¼‰
    'PROC': 0.5,       // æ‹–å»¶åŸå› 
    'EMO': 0.5,        // æƒ…ç»ªè§¦å‘
    'SAB': 0.5,        // è‡ªæˆ‘å¦¨ç¢
    'SOMA': 0.4,       // èº«å¿ƒååº”
  }

  let score = baseScores[memory.tag] || 0.5

  // ç½®ä¿¡åº¦è°ƒæ•´
  if (memory.confidence >= 0.8) {
    score += 0.1
  } else if (memory.confidence >= 0.7) {
    score += 0.05
  }

  // å†…å®¹å…·ä½“æ€§è°ƒæ•´ï¼ˆåŒ…å«æ•°å­—æˆ–ç‰¹å®šåè¯ï¼‰
  const hasSpecificDetails = /\d+|specific|always|never|every time|æ¯æ¬¡|æ€»æ˜¯|ä»ä¸/i.test(memory.content)
  if (hasSpecificDetails) {
    score += 0.1
  }

  // å†…å®¹é•¿åº¦è°ƒæ•´
  if (memory.content.length > 100) {
    score += 0.05
  }

  // ç¡®ä¿åœ¨ 0-1 èŒƒå›´å†…
  return Math.min(1, Math.max(0, score))
}

// è®°å¿†æå–çš„ç³»ç»Ÿæç¤ºè¯
// æ³¨æ„ï¼šSUCCESS è®°å½•å·²ä» tasks è¡¨è·å–ï¼Œä¸å†åœ¨æ­¤æå–
const EXTRACTION_PROMPT = `You are an AI Coach behavioral pattern extractor. Your job is to identify PATTERNS, PREFERENCES, and INSIGHTS from user conversations.

## EXTRACTION GUIDELINES

### DO NOT EXTRACT:
- Pure time/date mentions without context ("it's 4pm")
- Simple greetings or small talk ("hello", "hi")
- What AI said (only extract USER insights)

### EXTRACT THESE (Be generous - if in doubt, extract it):

**1. AI INTERACTION PREFERENCES** [Tag: PREF]
How user prefers to be communicated with.
Examples:
- "User dislikes being rushed or pressured"
- "User prefers gentle encouragement"
- "User wants shorter responses"

**2. PROCRASTINATION TRIGGERS** [Tag: PROC]
Reasons or excuses user gives for delaying tasks.
Examples:
- "User delays exercise because other work isn't done"
- "User feels they can't enjoy leisure until tasks are complete"
- "User says there's never enough time for reading"

**3. PSYCHOSOMATIC PATTERNS** [Tag: SOMA]
Physical feelings tied to activities.
Examples:
- "User feels tired when it's time to exercise"
- "User gets headaches before studying"

**4. EMOTIONAL TRIGGERS** [Tag: EMO]
Emotions or feelings about tasks/situations.
Examples:
- "User feels guilty about relaxing when work is unfinished"
- "User feels overwhelmed by too many tasks"
- "User feels anxious about not completing things"

**5. SELF-SABOTAGE PATTERNS** [Tag: SAB]
Behaviors that undermine user's goals.
Examples:
- "User checks phone before starting important tasks"
- "User makes excuses about time"
- "User keeps delaying things they want to do"

**6. TASK CONTEXT** [Tag: PROC]
Context about why user is doing or avoiding a task.
Examples:
- "User needs to finish other work before exercising"
- "User is procrastinating on gym, wants to shower first"

**7. EFFECTIVE ENCOURAGEMENT TECHNIQUES** [Tag: EFFECTIVE] â­ IMPORTANT FOR SUCCESSFUL SESSIONS
When task_completed=true, identify what AI said or did that helped user take action.
Look for:
- What AI said RIGHT BEFORE user decided to act
- Techniques that got positive response from user
- Phrases or approaches that broke through resistance

Examples:
- "Countdown technique worked - AI said 'let's count 3,2,1 and just start' and user responded positively"
- "Empathy before push - AI acknowledged user's tiredness first, then encouraged, user started the task"
- "Breaking task into tiny step worked - AI suggested 'just put on your shoes first' and user agreed"
- "Playful challenge effective - AI said 'bet you can't do 10 seconds' and user took the challenge"
- "Validation helped - AI said 'it's normal to feel resistance' and user felt understood then acted"

ONLY extract EFFECTIVE when:
- task_completed metadata is TRUE
- You can identify a specific AI technique that preceded user action
- User showed positive response or agreement before/after taking action

## OUTPUT FORMAT

Return a JSON array. Each memory:
- "content": The insight (be specific about WHAT and WHY)
- "tag": One of PREF, PROC, SOMA, EMO, SAB, EFFECTIVE
- "confidence": 0.3-1.0 (use lower values like 0.5-0.7 for single mentions, use 0.7+ for EFFECTIVE techniques that clearly worked)

**IMPORTANT: Be generous with extraction. Even single mentions can be valuable insights. If user expresses ANY emotion, reason for delay, or preference - extract it.**

Return empty array [] ONLY if conversation is purely greetings or completely meaningless.

## EXAMPLES

Input: User says "I can't enjoy leisure because my work isn't done"
Output:
[
  {
    "content": "User feels unable to enjoy leisure activities when work tasks are incomplete - guilt-driven work pattern",
    "tag": "EMO",
    "confidence": 0.7
  }
]

Input: User says "I'm procrastinating going to gym, need to shower first"
Output:
[
  {
    "content": "User delays going to gym by creating prerequisite tasks (showering first) - possible avoidance behavior",
    "tag": "PROC",
    "confidence": 0.6
  }
]

Input: User says "There's never enough time for reading"
Output:
[
  {
    "content": "User perceives lack of time for reading - may indicate low priority or avoidance pattern",
    "tag": "PROC",
    "confidence": 0.6
  }
]

Input: User just says "Hello, good morning"
Output:
[]`

// è®°å¿†åˆå¹¶çš„ç³»ç»Ÿæç¤ºè¯
const MERGE_PROMPT = `You are a memory consolidation expert. Your task is to merge multiple similar memories into ONE concise, comprehensive memory.

## RULES
1. Preserve ALL unique details from each memory
2. Remove redundancy and repetition
3. Keep the merged memory concise but complete
4. Maintain the same tag type
5. Use the highest confidence among the memories being merged

## OUTPUT FORMAT
Return a JSON object:
{
  "content": "The merged memory text",
  "confidence": 0.0-1.0
}

## EXAMPLE

Input memories:
1. "User feels unable to sleep when tasks are unfinished"
2. "User feels strong anxiety about going to bed because tasks feel unfinished"
3. "User becomes anxious whenever thinking about sleep due to incomplete work"

Output:
{
  "content": "User experiences strong anxiety about going to bed when tasks feel unfinished, which prevents them from falling asleep. The thought of incomplete work triggers stress that interferes with winding down.",
  "confidence": 0.9
}`

interface ExtractMemoryRequest {
  action: 'extract'
  userId: string
  messages: Array<{ role: string; content: string }>
  taskDescription?: string
  metadata?: Record<string, unknown>
}

interface SearchMemoryRequest {
  action: 'search'
  userId: string
  query: string
  limit?: number
}

interface GetMemoriesRequest {
  action: 'get'
  userId: string
  limit?: number
}

interface DeleteMemoryRequest {
  action: 'delete'
  memoryId: string
}

interface ConsolidateMemoryRequest {
  action: 'consolidate'
  userId: string
  tag?: string  // å¯é€‰ï¼šåªæ•´åˆç‰¹å®šæ ‡ç­¾çš„è®°å¿†
}

type MemoryRequest = ExtractMemoryRequest | SearchMemoryRequest | GetMemoriesRequest | DeleteMemoryRequest | ConsolidateMemoryRequest

interface ExtractedMemory {
  content: string
  tag: 'PREF' | 'PROC' | 'SOMA' | 'EMO' | 'SAB' | 'EFFECTIVE'
  confidence: number
}


interface ExistingMemory {
  id: string
  content: string
  tag: string
  confidence: number
  similarity: number
}

interface SaveResult {
  action: 'created' | 'merged' | 'skipped'
  memoryId: string
  content: string
  mergedFrom?: string[]
}

/**
 * ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡
 * ä½¿ç”¨ OpenAI SDK å…¼å®¹çš„ API æ ¼å¼
 */
async function generateEmbedding(text: string): Promise<number[]> {
  if (!EMBEDDING_API_KEY) {
    throw new Error('AZURE_EMBEDDING_API_KEY (or AZURE_AI_API_KEY) environment variable not set')
  }

  // OpenAI SDK å…¼å®¹æ ¼å¼
  // EMBEDDING_ENDPOINT åº”è¯¥æ˜¯å®Œæ•´çš„ base URLï¼Œå¦‚ï¼šhttps://xxx.azure.com/openai/v1/
  // å»æ‰æœ«å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç„¶ååŠ ä¸Š /embeddings
  const baseUrl = EMBEDDING_ENDPOINT.replace(/\/+$/, '')
  const apiUrl = `${baseUrl}/embeddings`

  console.log(`Calling embedding API: ${apiUrl}`)

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${EMBEDDING_API_KEY}`,  // OpenAI å…¼å®¹æ ¼å¼
    },
    body: JSON.stringify({
      model: EMBEDDING_MODEL,  // "text-embedding-3-small"
      input: text,
      dimensions: 1536,  // é™ç»´åˆ° 1536ï¼Œå…¼å®¹ HNSW ç´¢å¼•ï¼ˆæœ€å¤§ 2000 ç»´ï¼‰
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('Embedding API error:', response.status, error)
    throw new Error(`Embedding request failed: ${response.status} - ${error}`)
  }

  const result = await response.json()
  const embedding = result.data?.[0]?.embedding || []
  console.log(`Embedding generated successfully, dimensions: ${embedding.length}`)
  return embedding
}

/**
 * æŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰è®°å¿†
 */
async function findSimilarMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  embedding: number[],
  tag: string
): Promise<ExistingMemory[]> {
  // ä½¿ç”¨æ•°æ®åº“å‡½æ•°è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
  const { data, error } = await supabase.rpc('search_similar_memories', {
    p_user_id: userId,
    p_embedding: JSON.stringify(embedding),
    p_tag: tag,
    p_threshold: SIMILARITY_THRESHOLD,
    p_limit: 5,
  })

  if (error) {
    console.error('Similar memory search error:', error)
    // å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼ˆè¿ç§»æœªè¿è¡Œï¼‰ï¼Œè¿”å›ç©ºæ•°ç»„
    return []
  }

  return data || []
}

/**
 * ä½¿ç”¨ LLM åˆå¹¶å¤šæ¡ç›¸ä¼¼è®°å¿†
 */
async function mergeMemoriesWithAI(
  memories: Array<{ content: string; confidence: number }>
): Promise<{ content: string; confidence: number }> {
  if (!AZURE_API_KEY) {
    throw new Error('AZURE_AI_API_KEY environment variable not set')
  }

  const memoriesText = memories
    .map((m, i) => `${i + 1}. "${m.content}"`)
    .join('\n')

  const apiUrl = `${AZURE_ENDPOINT}/openai/v1/chat/completions`

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AZURE_API_KEY}`,
    },
    body: JSON.stringify({
      model: MODEL_NAME,
      messages: [
        { role: 'system', content: MERGE_PROMPT },
        { role: 'user', content: `Merge these memories:\n${memoriesText}` }
      ],
      max_completion_tokens: 500,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('Merge API error:', error)
    throw new Error(`Merge request failed: ${response.status}`)
  }

  const result = await response.json()
  const content = result.choices?.[0]?.message?.content

  try {
    const parsed = JSON.parse(content)
    return {
      content: parsed.content,
      confidence: Math.max(...memories.map(m => m.confidence), parsed.confidence || 0.8),
    }
  } catch (e) {
    console.error('Failed to parse merge response:', content, e)
    // å›é€€ï¼šä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„é‚£æ¡
    const best = memories.reduce((a, b) => a.confidence > b.confidence ? a : b)
    return best
  }
}

/**
 * è°ƒç”¨ Azure AI æå–è®°å¿†
 * @param messages å¯¹è¯æ¶ˆæ¯
 * @param taskDescription ä»»åŠ¡æè¿°
 * @param taskCompleted ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆï¼ˆç”¨äºæå– EFFECTIVE æ¿€åŠ±æ–¹å¼ï¼‰
 */
async function extractMemoriesWithAI(
  messages: Array<{ role: string; content: string }>,
  taskDescription?: string,
  taskCompleted?: boolean
): Promise<ExtractedMemory[]> {
  if (!AZURE_API_KEY) {
    throw new Error('AZURE_AI_API_KEY environment variable not set')
  }

  // è¿‡æ»¤æ‰ç©ºæ¶ˆæ¯æˆ–æ— æ•ˆæ¶ˆæ¯
  const validMessages = messages.filter(m => m && m.content && typeof m.content === 'string' && m.content.trim())

  console.log(`Received ${messages.length} messages, ${validMessages.length} valid`)

  if (validMessages.length === 0) {
    console.log('No valid messages to process')
    return []
  }

  // åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯ï¼ˆå› ä¸ºæµå¼è¾“å‡ºä¼šæŠŠä¸€å¥è¯åˆ†æˆå¤šæ¡æ¶ˆæ¯ï¼‰
  const mergedMessages: Array<{ role: string; content: string }> = []
  for (const msg of validMessages) {
    const lastMsg = mergedMessages[mergedMessages.length - 1]
    if (lastMsg && lastMsg.role === msg.role) {
      // åˆå¹¶åˆ°ä¸Šä¸€æ¡æ¶ˆæ¯
      lastMsg.content += msg.content
    } else {
      // æ–°è§’è‰²ï¼Œåˆ›å»ºæ–°æ¶ˆæ¯
      mergedMessages.push({ role: msg.role, content: msg.content })
    }
  }

  console.log(`Merged to ${mergedMessages.length} messages`)

  // æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ŒåŒ…å«å¯¹è¯å†…å®¹
  const conversationText = mergedMessages
    .map(m => `${m.role?.toUpperCase() || 'UNKNOWN'}: ${m.content}`)
    .join('\n')

  console.log(`Conversation text length: ${conversationText.length} chars`)

  // æ„å»ºç”¨æˆ·æç¤ºï¼ŒåŒ…å«ä»»åŠ¡å®ŒæˆçŠ¶æ€
  let userPrompt = ''
  if (taskDescription) {
    userPrompt += `Task context: "${taskDescription}"\n`
  }
  if (taskCompleted !== undefined) {
    userPrompt += `task_completed: ${taskCompleted}\n`
    if (taskCompleted) {
      userPrompt += `â­ This was a SUCCESSFUL session - user completed the task! Look for EFFECTIVE encouragement techniques that helped.\n`
    }
  }
  userPrompt += `\nConversation:\n${conversationText}`

  // è°ƒç”¨ Azure AI Foundry - ä½¿ç”¨ OpenAI å…¼å®¹çš„ REST API
  // URL: {endpoint}/openai/v1/chat/completions
  // è®¤è¯: Authorization: Bearer {api_key}
  const apiUrl = `${AZURE_ENDPOINT}/openai/v1/chat/completions`

  console.log(`Calling Azure AI: ${apiUrl} with model: ${MODEL_NAME}`)

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AZURE_API_KEY}`,
    },
    body: JSON.stringify({
      model: MODEL_NAME,  // éœ€è¦åœ¨ body ä¸­æŒ‡å®š model
      messages: [
        { role: 'system', content: EXTRACTION_PROMPT },
        { role: 'user', content: userPrompt }
      ],
      max_completion_tokens: 1000,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    console.error('Azure AI error:', error)
    throw new Error(`Azure AI request failed: ${response.status} ${error}`)
  }

  const result = await response.json()
  const content = result.choices?.[0]?.message?.content

  if (!content) {
    console.log('No content in AI response')
    return []
  }

  try {
    const parsed = JSON.parse(content)
    // å¤„ç†å¯èƒ½çš„ä¸åŒæ ¼å¼
    const memories = Array.isArray(parsed) ? parsed : (parsed.memories || parsed.results || [])
    return memories.filter((m: ExtractedMemory) => m.content && m.tag && m.confidence >= 0.3)
  } catch (e) {
    console.error('Failed to parse AI response:', content, e)
    return []
  }
}

/**
 * ä¿å­˜æˆ–åˆå¹¶è®°å¿†åˆ° Supabaseï¼ˆå« Update Phase é€»è¾‘ï¼‰
 *
 * æµç¨‹ï¼š
 * 1. ä¸ºæ¯æ¡æ–°è®°å¿†ç”Ÿæˆ embedding
 * 2. æŸ¥æ‰¾ç›¸ä¼¼çš„ç°æœ‰è®°å¿†
 * 3. å¦‚æœæ‰¾åˆ°ç›¸ä¼¼è®°å¿† â†’ åˆå¹¶å¹¶æ›´æ–°
 * 4. å¦‚æœæ²¡æœ‰ç›¸ä¼¼è®°å¿† â†’ åˆ›å»ºæ–°è®°å¿†
 */
async function saveOrMergeMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  memories: ExtractedMemory[],
  taskDescription?: string,
  metadata?: Record<string, unknown>
): Promise<{ saved: number; merged: number; results: SaveResult[] }> {
  if (memories.length === 0) {
    return { saved: 0, merged: 0, results: [] }
  }

  const results: SaveResult[] = []
  let savedCount = 0
  let mergedCount = 0

  for (const memory of memories) {
    try {
      // 1. ç”Ÿæˆ embedding
      console.log(`Generating embedding for: ${memory.content.substring(0, 50)}...`)
      const embedding = await generateEmbedding(memory.content)

      // è®¡ç®—åˆå§‹é‡è¦æ€§è¯„åˆ†
      const importanceScore = calculateImportanceScore(memory)
      console.log(`ğŸ“Š Importance score for "${memory.tag}": ${importanceScore.toFixed(2)}`)

      if (embedding.length === 0) {
        console.warn('Failed to generate embedding, saving without dedup')
        // å›é€€åˆ°ç®€å•æ’å…¥
        const { data } = await supabase
          .from('user_memories')
          .insert({
            user_id: userId,
            content: memory.content,
            tag: memory.tag,
            confidence: memory.confidence,
            importance_score: importanceScore,
            task_name: taskDescription || null,
            metadata: metadata || {},
          })
          .select()
          .single()

        if (data) {
          results.push({ action: 'created', memoryId: data.id, content: memory.content })
          savedCount++
        }
        continue
      }

      // 2. æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†
      const similarMemories = await findSimilarMemories(supabase, userId, embedding, memory.tag)
      console.log(`Found ${similarMemories.length} similar memories for tag ${memory.tag}`)

      if (similarMemories.length > 0) {
        // 3. æœ‰ç›¸ä¼¼è®°å¿† â†’ åˆå¹¶
        const allMemories = [
          { content: memory.content, confidence: memory.confidence },
          ...similarMemories.map(m => ({ content: m.content, confidence: m.confidence }))
        ]

        console.log(`Merging ${allMemories.length} memories...`)
        const merged = await mergeMemoriesWithAI(allMemories)

        // ç”Ÿæˆåˆå¹¶åå†…å®¹çš„æ–° embedding
        const mergedEmbedding = await generateEmbedding(merged.content)

        // æ›´æ–°æœ€ç›¸ä¼¼çš„é‚£æ¡è®°å¿†ï¼ˆä¿ç•™å…¶ IDï¼‰
        const targetMemory = similarMemories[0]
        const mergedFromIds = similarMemories.map(m => m.id)

        // åˆå¹¶åé‡è¦æ€§æå‡ï¼ˆå¤šæ¬¡å‡ºç°è¯´æ˜æ›´é‡è¦ï¼‰
        const mergedImportance = Math.min(1, importanceScore + 0.1 * (similarMemories.length - 1))

        const { data: _data, error } = await supabase
          .from('user_memories')
          .update({
            content: merged.content,
            confidence: merged.confidence,
            importance_score: mergedImportance,
            embedding: JSON.stringify(mergedEmbedding),
            task_name: taskDescription || null,
            merged_from: mergedFromIds,
            version: (targetMemory as unknown as { version?: number }).version ? (targetMemory as unknown as { version: number }).version + 1 : 2,
            metadata: {
              ...metadata,
              lastMergedAt: new Date().toISOString(),
              mergeCount: (similarMemories.length + 1),
            },
          })
          .eq('id', targetMemory.id)
          .select()
          .single()

        if (error) {
          console.error('Failed to update merged memory:', error)
          continue
        }

        // åˆ é™¤å…¶ä»–è¢«åˆå¹¶çš„è®°å¿†ï¼ˆé™¤äº†ç›®æ ‡è®°å¿†ï¼‰
        if (similarMemories.length > 1) {
          const idsToDelete = similarMemories.slice(1).map(m => m.id)
          await supabase
            .from('user_memories')
            .delete()
            .in('id', idsToDelete)
          console.log(`Deleted ${idsToDelete.length} merged source memories`)
        }

        results.push({
          action: 'merged',
          memoryId: targetMemory.id,
          content: merged.content,
          mergedFrom: mergedFromIds,
        })
        mergedCount++

      } else {
        // 4. æ²¡æœ‰ç›¸ä¼¼è®°å¿† â†’ åˆ›å»ºæ–°è®°å¿†
        const { data, error } = await supabase
          .from('user_memories')
          .insert({
            user_id: userId,
            content: memory.content,
            tag: memory.tag,
            confidence: memory.confidence,
            importance_score: importanceScore,
            embedding: JSON.stringify(embedding),
            task_name: taskDescription || null,
            metadata: metadata || {},
          })
          .select()
          .single()

        if (error) {
          console.error('Failed to save new memory:', error)
          continue
        }

        results.push({ action: 'created', memoryId: data.id, content: memory.content })
        savedCount++
      }

    } catch (err) {
      console.error(`Error processing memory: ${memory.content.substring(0, 50)}...`, err)

      // Fallbackï¼šå³ä½¿ embedding å¤±è´¥ï¼Œä¹Ÿå°è¯•ä¿å­˜è®°å¿†ï¼ˆä¸åšå»é‡ï¼‰
      try {
        console.log('Attempting fallback save without embedding...')
        const fallbackImportance = calculateImportanceScore(memory)
        const { data, error } = await supabase
          .from('user_memories')
          .insert({
            user_id: userId,
            content: memory.content,
            tag: memory.tag,
            confidence: memory.confidence,
            importance_score: fallbackImportance,
            task_name: taskDescription || null,
            metadata: {
              ...metadata,
              embeddingFailed: true,
              embeddingError: String(err),
            },
          })
          .select()
          .single()

        if (data) {
          console.log(`Fallback save successful: ${data.id}`)
          results.push({ action: 'created', memoryId: data.id, content: memory.content })
          savedCount++
        } else if (error) {
          console.error('Fallback save failed:', error)
        }
      } catch (fallbackErr) {
        console.error('Fallback save exception:', fallbackErr)
      }
    }
  }

  return { saved: savedCount, merged: mergedCount, results }
}

/**
 * æœç´¢ç›¸å…³è®°å¿†
 */
async function searchMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  query: string,
  limit = 10
) {
  // ç®€å•çš„æ–‡æœ¬æœç´¢ï¼Œåç»­å¯ä»¥æ”¹æˆå‘é‡æœç´¢
  const { data, error } = await supabase
    .from('user_memories')
    .select('*')
    .eq('user_id', userId)
    .ilike('content', `%${query}%`)
    .order('created_at', { ascending: false })
    .limit(limit)

  if (error) {
    throw new Error(`Failed to search memories: ${error.message}`)
  }

  return data || []
}

/**
 * è·å–ç”¨æˆ·æ‰€æœ‰è®°å¿†
 */
async function getMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  limit = 50
) {
  const { data, error } = await supabase
    .from('user_memories')
    .select('*')
    .eq('user_id', userId)
    .order('created_at', { ascending: false })
    .limit(limit)

  if (error) {
    throw new Error(`Failed to get memories: ${error.message}`)
  }

  return data || []
}

/**
 * åˆ é™¤è®°å¿†
 */
async function deleteMemory(
  supabase: ReturnType<typeof createClient>,
  memoryId: string
) {
  const { error } = await supabase
    .from('user_memories')
    .delete()
    .eq('id', memoryId)

  if (error) {
    throw new Error(`Failed to delete memory: ${error.message}`)
  }

  return { success: true, memoryId }
}

/**
 * æ•´åˆç°æœ‰è®°å¿†ï¼ˆæ¸…ç†é‡å¤ï¼‰
 * å¯¹ç”¨æˆ·çš„æ¯ä¸ªæ ‡ç­¾ç±»åˆ«ï¼Œæ‰¾å‡ºç›¸ä¼¼è®°å¿†å¹¶åˆå¹¶
 */
async function consolidateMemories(
  supabase: ReturnType<typeof createClient>,
  userId: string,
  targetTag?: string
): Promise<{ processed: number; merged: number; deleted: number }> {
  // æ•´åˆè¡Œä¸ºæ¨¡å¼è®°å¿†ï¼ŒåŒ…æ‹¬ EFFECTIVEï¼ˆæœ‰æ•ˆæ¿€åŠ±æ–¹å¼ï¼‰
  const tags = targetTag ? [targetTag] : ['PREF', 'PROC', 'SOMA', 'EMO', 'SAB', 'EFFECTIVE']

  let totalProcessed = 0
  let totalMerged = 0
  let totalDeleted = 0

  // æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆæ¯” embedding é˜ˆå€¼ä½ä¸€äº›ï¼Œå› ä¸ºå‡†ç¡®æ€§è¾ƒä½ï¼‰
  const TEXT_SIMILARITY_THRESHOLD = 0.4

  for (const tag of tags) {
    // è·å–è¯¥æ ‡ç­¾çš„æ‰€æœ‰è®°å¿†
    console.log(`Querying memories for user ${userId}, tag ${tag}...`)

    const { data: memories, error } = await supabase
      .from('user_memories')
      .select('*')
      .eq('user_id', userId)
      .eq('tag', tag)
      .order('created_at', { ascending: true })

    if (error) {
      console.error(`Query error for tag ${tag}:`, error)
      continue
    }

    if (!memories || memories.length === 0) {
      console.log(`No memories found for tag ${tag}`)
      continue
    }

    if (memories.length < 2) {
      console.log(`Only ${memories.length} memory for tag ${tag}, skipping (need at least 2 to consolidate)`)
      totalProcessed += memories.length
      continue
    }

    console.log(`Processing ${memories.length} memories for tag ${tag}`)
    totalProcessed += memories.length

    // å°è¯•ä¸ºæ²¡æœ‰ embedding çš„è®°å¿†ç”Ÿæˆ embeddingï¼ˆä½†å¤±è´¥ä¸é˜»å¡æµç¨‹ï¼‰
    let embeddingAvailable = false
    for (const memory of memories) {
      if (!memory.embedding) {
        try {
          const embedding = await generateEmbedding(memory.content)
          if (embedding && embedding.length > 0) {
            await supabase
              .from('user_memories')
              .update({ embedding: JSON.stringify(embedding) })
              .eq('id', memory.id)
            memory.embedding = embedding
            embeddingAvailable = true
          }
        } catch (err) {
          console.warn(`Embedding generation failed for ${memory.id}, will use text similarity:`, err)
        }
      } else {
        embeddingAvailable = true
      }
    }

    console.log(`Embedding available: ${embeddingAvailable}, using ${embeddingAvailable ? 'vector' : 'text'} similarity`)

    // æ‰¾å‡ºç›¸ä¼¼ç»„ï¼ˆä½¿ç”¨ç®€å•çš„è´ªå¿ƒèšç±»ï¼‰
    const processed = new Set<string>()
    const groups: typeof memories[] = []

    for (const memory of memories) {
      if (processed.has(memory.id)) continue

      const group = [memory]
      processed.add(memory.id)

      // æ‰¾å‡ºä¸å½“å‰è®°å¿†ç›¸ä¼¼çš„å…¶ä»–è®°å¿†
      for (const other of memories) {
        if (processed.has(other.id)) continue

        let similarity = 0

        // ä¼˜å…ˆä½¿ç”¨ embedding ç›¸ä¼¼åº¦
        if (memory.embedding && other.embedding) {
          try {
            similarity = cosineSimilarity(
              typeof memory.embedding === 'string' ? JSON.parse(memory.embedding) : memory.embedding,
              typeof other.embedding === 'string' ? JSON.parse(other.embedding) : other.embedding
            )
            if (similarity >= SIMILARITY_THRESHOLD) {
              group.push(other)
              processed.add(other.id)
              console.log(`  Embedding match: "${memory.content.substring(0, 30)}..." ~ "${other.content.substring(0, 30)}..." (${(similarity * 100).toFixed(1)}%)`)
            }
          } catch {
            console.warn('Failed to parse embeddings, falling back to text similarity')
            similarity = textSimilarity(memory.content, other.content)
            if (similarity >= TEXT_SIMILARITY_THRESHOLD) {
              group.push(other)
              processed.add(other.id)
              console.log(`  Text match: "${memory.content.substring(0, 30)}..." ~ "${other.content.substring(0, 30)}..." (${(similarity * 100).toFixed(1)}%)`)
            }
          }
        } else {
          // å›é€€åˆ°æ–‡æœ¬ç›¸ä¼¼åº¦
          similarity = textSimilarity(memory.content, other.content)
          if (similarity >= TEXT_SIMILARITY_THRESHOLD) {
            group.push(other)
            processed.add(other.id)
            console.log(`  Text match: "${memory.content.substring(0, 30)}..." ~ "${other.content.substring(0, 30)}..." (${(similarity * 100).toFixed(1)}%)`)
          }
        }
      }

      if (group.length > 1) {
        groups.push(group)
      }
    }

    console.log(`Found ${groups.length} groups of similar memories for tag ${tag}`)

    // åˆå¹¶æ¯ä¸ªç›¸ä¼¼ç»„
    for (const group of groups) {
      try {
        // ç”¨ LLM åˆå¹¶
        const merged = await mergeMemoriesWithAI(
          group.map(m => ({ content: m.content, confidence: m.confidence }))
        )

        // å°è¯•ç”Ÿæˆæ–°çš„ embeddingï¼ˆå¯é€‰ï¼Œå¤±è´¥ä¸é˜»å¡ï¼‰
        let mergedEmbedding: number[] | null = null
        try {
          mergedEmbedding = await generateEmbedding(merged.content)
          if (!mergedEmbedding || mergedEmbedding.length === 0) {
            mergedEmbedding = null
          }
        } catch (embeddingErr) {
          console.warn('Failed to generate embedding for merged content, proceeding without:', embeddingErr)
        }

        // æ›´æ–°ç¬¬ä¸€æ¡è®°å¿†
        const targetId = group[0].id
        const mergedFromIds = group.map(m => m.id)

        // æ„å»ºæ›´æ–°å¯¹è±¡ï¼ˆembedding å¯é€‰ï¼‰
        const updateData: Record<string, unknown> = {
          content: merged.content,
          confidence: merged.confidence,
          merged_from: mergedFromIds,
          metadata: {
            consolidatedAt: new Date().toISOString(),
            mergeCount: group.length,
          },
        }
        if (mergedEmbedding) {
          updateData.embedding = JSON.stringify(mergedEmbedding)
        }

        await supabase
          .from('user_memories')
          .update(updateData)
          .eq('id', targetId)

        // åˆ é™¤å…¶ä»–è®°å¿†
        const idsToDelete = group.slice(1).map(m => m.id)
        if (idsToDelete.length > 0) {
          await supabase
            .from('user_memories')
            .delete()
            .in('id', idsToDelete)

          totalDeleted += idsToDelete.length
        }

        totalMerged++
        console.log(`Merged ${group.length} memories into one (tag: ${tag})`)

      } catch (err) {
        console.error(`Failed to merge group:`, err)
      }
    }
  }

  return { processed: totalProcessed, merged: totalMerged, deleted: totalDeleted }
}

/**
 * è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
 */
function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) return 0

  let dotProduct = 0
  let normA = 0
  let normB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    normA += a[i] * a[i]
    normB += b[i] * b[i]
  }

  if (normA === 0 || normB === 0) return 0
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
}

/**
 * è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ (Jaccard + å…³é”®è¯é‡å )
 * ç”¨äºåœ¨æ²¡æœ‰ embedding æ—¶ä½œä¸ºå›é€€æ–¹æ¡ˆ
 */
function textSimilarity(text1: string, text2: string): number {
  // ç®€å•åˆ†è¯
  const tokenize = (text: string) => {
    return text.toLowerCase()
      .replace(/[^\w\s]/g, ' ')
      .split(/\s+/)
      .filter(t => t.length > 2)
  }

  const tokens1 = new Set(tokenize(text1))
  const tokens2 = new Set(tokenize(text2))

  // Jaccard ç›¸ä¼¼åº¦
  const intersection = [...tokens1].filter(t => tokens2.has(t)).length
  const union = new Set([...tokens1, ...tokens2]).size

  if (union === 0) return 0
  return intersection / union
}

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    // åˆå§‹åŒ– Supabase client
    const supabaseUrl = Deno.env.get('SUPABASE_URL')!
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    const supabase = createClient(supabaseUrl, supabaseServiceKey)

    const body: MemoryRequest = await req.json()

    let result: unknown

    switch (body.action) {
      case 'extract': {
        const { userId, messages, taskDescription, metadata } = body as ExtractMemoryRequest
        if (!userId || !messages || messages.length === 0) {
          throw new Error('Missing required fields: userId, messages')
        }

        console.log(`Extracting memories for user: ${userId}, messages: ${messages.length}`)
        const taskCompleted = (metadata as Record<string, unknown>)?.task_completed === true
        console.log(`Task completed: ${taskCompleted}, duration: ${(metadata as Record<string, unknown>)?.actual_duration_minutes} min`)

        // 1. ç”¨ AI æå–è®°å¿†ï¼ˆä¼ å…¥ä»»åŠ¡å®ŒæˆçŠ¶æ€ï¼Œç”¨äºæå– EFFECTIVE æ¿€åŠ±æ–¹å¼ï¼‰
        const extractedMemories = await extractMemoriesWithAI(messages, taskDescription, taskCompleted)
        console.log(`Extracted ${extractedMemories.length} memories (task_completed=${taskCompleted})`)

        // æ³¨æ„ï¼šSUCCESS è®°å½•ä¸å†åœ¨æ­¤å¤„ç†ï¼Œä»»åŠ¡å®ŒæˆçŠ¶æ€å·²åœ¨ tasks è¡¨ä¸­è·Ÿè¸ª

        // 2. ä¿å­˜æˆ–åˆå¹¶åˆ° Supabase (Update Phase)
        if (extractedMemories.length > 0) {
          const saveResult = await saveOrMergeMemories(supabase, userId, extractedMemories, taskDescription, {
            ...metadata,
            taskDescription,
            extractedAt: new Date().toISOString(),
          })
          result = {
            extracted: extractedMemories.length,
            saved: saveResult.saved,
            merged: saveResult.merged,
            results: saveResult.results,
            memories: extractedMemories
          }
        } else {
          result = { extracted: 0, saved: 0, merged: 0, results: [], memories: [] }
        }
        break
      }

      case 'search': {
        const { userId, query, limit } = body as SearchMemoryRequest
        if (!userId || !query) {
          throw new Error('Missing required fields: userId, query')
        }
        console.log(`Searching memories for user: ${userId}, query: ${query}`)
        result = await searchMemories(supabase, userId, query, limit)
        break
      }

      case 'get': {
        const { userId, limit } = body as GetMemoriesRequest
        if (!userId) {
          throw new Error('Missing required field: userId')
        }
        console.log(`Getting memories for user: ${userId}`)
        result = await getMemories(supabase, userId, limit)
        break
      }

      case 'delete': {
        const { memoryId } = body as DeleteMemoryRequest
        if (!memoryId) {
          throw new Error('Missing required field: memoryId')
        }
        console.log(`Deleting memory: ${memoryId}`)
        result = await deleteMemory(supabase, memoryId)
        break
      }

      case 'consolidate': {
        const { userId, tag } = body as ConsolidateMemoryRequest
        if (!userId) {
          throw new Error('Missing required field: userId')
        }
        console.log(`=== CONSOLIDATE REQUEST ===`)
        console.log(`userId: ${userId}`)
        console.log(`userId type: ${typeof userId}`)
        console.log(`tag filter: ${tag || 'all'}`)

        // å…ˆéªŒè¯ç”¨æˆ·æ˜¯å¦æœ‰è®°å¿†
        const { data: checkData, error: checkError } = await supabase
          .from('user_memories')
          .select('id, tag')
          .eq('user_id', userId)
          .limit(10)

        console.log(`Pre-check: found ${checkData?.length || 0} memories for this user`)
        if (checkError) {
          console.error('Pre-check error:', checkError)
        }
        if (checkData && checkData.length > 0) {
          console.log('Sample memories:', checkData.slice(0, 3))
        }

        result = await consolidateMemories(supabase, userId, tag)
        console.log(`Consolidate result:`, result)
        break
      }

      default:
        throw new Error(`Unknown action: ${(body as { action: string }).action}`)
    }

    return new Response(
      JSON.stringify(result),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )

  } catch (error) {
    console.error('Memory extractor error:', error)
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  }
})
